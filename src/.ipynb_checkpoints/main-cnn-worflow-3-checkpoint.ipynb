{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2db6040-ad1a-4565-853b-af70e08ac16c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d04e172b-3781-4e86-8b7d-51e002a20af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras\n",
      "  Downloading keras-2.11.0-py2.py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m0m\n",
      "\u001b[?25hInstalling collected packages: keras\n",
      "Successfully installed keras-2.11.0\n"
     ]
    }
   ],
   "source": [
    "!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9ab9e00-52b1-421c-802c-28322be85014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.11.0-cp39-cp39-macosx_10_14_x86_64.whl (244.3 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.3/244.3 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting gast<=0.4.0,>=0.2.1\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting protobuf<3.20,>=3.9.2\n",
      "  Downloading protobuf-3.19.6-cp39-cp39-macosx_10_9_x86_64.whl (980 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m980.5/980.5 kB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting termcolor>=1.1.0\n",
      "  Downloading termcolor-2.2.0-py3-none-any.whl (6.6 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/konstantinoskalaitzidis/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (4.3.0)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting absl-py>=1.0.0\n",
      "  Downloading absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.5/126.5 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.20 in /Users/konstantinoskalaitzidis/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (1.21.5)\n",
      "Collecting tensorflow-estimator<2.12,>=2.11.0\n",
      "  Downloading tensorflow_estimator-2.11.0-py2.py3-none-any.whl (439 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m439.2/439.2 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorboard<2.12,>=2.11\n",
      "  Downloading tensorboard-2.11.2-py3-none-any.whl (6.0 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting libclang>=13.0.0\n",
      "  Downloading libclang-15.0.6.1-py2.py3-none-macosx_10_9_x86_64.whl (25.0 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25.0/25.0 MB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /Users/konstantinoskalaitzidis/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (63.4.1)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Users/konstantinoskalaitzidis/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: keras<2.12,>=2.11.0 in /Users/konstantinoskalaitzidis/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (2.11.0)\n",
      "Collecting flatbuffers>=2.0\n",
      "  Downloading flatbuffers-23.1.21-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/konstantinoskalaitzidis/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (1.16.0)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.31.0-cp39-cp39-macosx_10_14_x86_64.whl (1.6 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in /Users/konstantinoskalaitzidis/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (21.3)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /Users/konstantinoskalaitzidis/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (3.7.0)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.51.3-cp39-cp39-macosx_10_10_universal2.whl (8.2 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in /Users/konstantinoskalaitzidis/opt/anaconda3/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.16.2-py2.py3-none-any.whl (177 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.2/177.2 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2.21.0 in /Users/konstantinoskalaitzidis/opt/anaconda3/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.28.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/konstantinoskalaitzidis/opt/anaconda3/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (3.3.4)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/konstantinoskalaitzidis/opt/anaconda3/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.0.3)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-macosx_10_9_x86_64.whl (3.5 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/konstantinoskalaitzidis/opt/anaconda3/lib/python3.9/site-packages (from packaging->tensorflow) (3.0.9)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.3.0-py3-none-any.whl (9.3 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/konstantinoskalaitzidis/opt/anaconda3/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (0.2.8)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/konstantinoskalaitzidis/opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/konstantinoskalaitzidis/opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (2022.9.24)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/konstantinoskalaitzidis/opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/konstantinoskalaitzidis/opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (3.3)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Users/konstantinoskalaitzidis/opt/anaconda3/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (0.4.8)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.7/151.7 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tensorboard-plugin-wit, libclang, flatbuffers, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, rsa, protobuf, opt-einsum, oauthlib, grpcio, google-pasta, gast, cachetools, astunparse, absl-py, requests-oauthlib, google-auth, google-auth-oauthlib, tensorboard, tensorflow\n",
      "Successfully installed absl-py-1.4.0 astunparse-1.6.3 cachetools-5.3.0 flatbuffers-23.1.21 gast-0.4.0 google-auth-2.16.2 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.51.3 libclang-15.0.6.1 oauthlib-3.2.2 opt-einsum-3.3.0 protobuf-3.19.6 requests-oauthlib-1.3.1 rsa-4.9 tensorboard-2.11.2 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.11.0 tensorflow-estimator-2.11.0 tensorflow-io-gcs-filesystem-0.31.0 termcolor-2.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd47a7d5-e6f1-4c67-9142-bdb96553bc33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-03 11:52:39.441288: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7ecfb3-3fbd-4f9f-9830-ca10e101d4a9",
   "metadata": {},
   "source": [
    "- Conv2D: This class is used to define a 2D convolutional layer in a CNN model. This layer is responsible for extracting features from the input data.\n",
    "- MaxPooling2D: This class is used to define a 2D max pooling layer in a CNN model. This layer is responsible for down-sampling the input data.\n",
    "- Flatten: This class is used to flatten the output of the convolutional and max pooling layers, before passing it to the dense layers.\n",
    "- Dense: This class is used to define a fully connected layer in a neural network model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "022dfcdb-4f67-422b-9854-2df348325e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63b493b-640a-4d89-8995-52aa1c87f39a",
   "metadata": {},
   "source": [
    "The MNIST dataset is a dataset of handwritten digits, which is often used as a benchmark for training and evaluating machine learning models. The dataset contains 60,000 training images and 10,000 test images of handwritten digits, along with their corresponding labels (i.e., the digit that is written in each image)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20c851ee-b9fe-47e2-95f3-e4cbc11a603c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863f2487-754c-46ae-89e5-ec7bece52a76",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847c75f2-0a69-4d5c-894d-54c55cc64f10",
   "metadata": {},
   "source": [
    "Loading the MNIST data. Returns a tuple (used to store multiple items in a single variable) training and test sets, \n",
    "where X_train and X_test are the image data, and y_train and y_test are the corresponding labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6cdce83-0a42-4fe4-bc6f-6b5ba49c39fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11490434/11490434 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9984ca8-1f2e-4bf3-bff9-b5f5ce475adb",
   "metadata": {},
   "source": [
    "Reshape and normalize the data\n",
    "The CNNs expect a 4D tensor with the shape (batch_size, height, width, channels). \n",
    "This step is adding the missing channel dimension to the data, which is 1 in this case as the images are grayscale.\n",
    "In the case of an RGB (Red, Green, Blue) image it would have been 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63784656-d5d2-42f0-992c-894ebac817f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(60000, 28, 28, 1)\n",
    "X_test = X_test.reshape(10000, 28, 28, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4597a6a-bc19-4eaf-bd52-8d7b5f533b5f",
   "metadata": {},
   "source": [
    "To ensure that the pixel values are in the range of 0 to 1, which is a common preprocessing step for image data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b719e493-bdca-4931-9963-c9856da07e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255 # why 255?\n",
    "X_test /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e36303f-d45c-42a3-a320-d726e994d4cc",
   "metadata": {},
   "source": [
    "Convert the labels to categorical\n",
    "This function converts the integer labels to a binary format where each label is represented as a one-hot encoded vector. \n",
    "This step is necessary because the final output layer of the network uses a softmax activation function \n",
    "which expects the labels to be in this format. \n",
    "The input argument 10 means that we have 10 classes (0-9 digits)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4524475e-5627-48e7-89ac-d36c43fede3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9adcd4b-f0f5-48c8-a31c-af642fadba11",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Build the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81af5f69-ed46-4c53-b29e-ed9da958f048",
   "metadata": {},
   "source": [
    "Create a sequential model\n",
    "A sequential model is a linear stack of layers, where the output of one layer is the input of the next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "08b82cf4-dc1d-4625-849d-cc0563cdce3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-03 11:57:51.084414: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209d3c69-e177-4554-a5f3-370d72d4c8d1",
   "metadata": {},
   "source": [
    "Add a convolutional layer with 32 filters, a kernel size of 3x3, and a ReLU activation function\n",
    "The ReLU activation function is a simple equation that takes the input of a neuron and returns the input if it is positive, \n",
    "and returns 0 if it is negative. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13715549-7452-414d-ae28-91fbe74799e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1))) # input is a 28x28 image with 1 color channel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f79f5d8-ce1c-4e01-8e69-5fd6d5a955ea",
   "metadata": {},
   "source": [
    "Add a max pooling layer with a pool size of 2x2\n",
    "This layer applies a max operation over a 2x2 window of the input, reducing the spatial dimensions of the input by half."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0149c388-57fa-4f65-bdee-a8a727f0b99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Add a convolutional layer with 64 filters, a kernel size of 3x3, and a ReLU activation function\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "\n",
    "# Add a max pooling layer with a pool size of 2x2\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Flatten the output from the previous layers\n",
    "model.add(Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386991f0-5685-496f-b725-da98d8e91e96",
   "metadata": {},
   "source": [
    "Add a fully connected layer with 128 units and a ReLU activation function\n",
    "This layer has 128 neurons and it is fully connected to the previous layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f8c9cba5-b845-478f-bf2e-7938de693a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(128, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21bb36c-7b3f-4298-975b-6a065a9eea21",
   "metadata": {},
   "source": [
    "Add a final output layer with 10 units and a softmax activation function\n",
    "The softmax function is used to convert the output of the final layer into probability distribution over 10 possible classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f88ff7ce-3108-48f3-8ec7-c2d349363884",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d563a278-453f-4c6b-a995-045a5fcb1809",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a08a374-59f7-470e-8251-18c42ca1a9d4",
   "metadata": {},
   "source": [
    "Compiling the model with a categorical crossentropy loss function and an Adam optimizer\n",
    "- loss: This argument specifies the loss function that the model should use to measure its performance during training. A loss function is a mathematical equation that measures how well the model is able to make predictions. \n",
    "- optimizer: This argument specifies the optimization algorithm that the model should use to update its weights during training.\n",
    "- metrics: this argument specifies the metric(s) that the model should use to evaluate its performance during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcaf081b-6993-4f6f-a324-b107c2a23ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e8edd5-8899-4c5f-a962-ddff37fef356",
   "metadata": {},
   "source": [
    "Train the model on the training data\n",
    "- X_train and y_train: These arguments specify the training data and labels. X_train is the input data and y_train is the corresponding target data.\n",
    "- epochs: This argument specifies the number of times the model should iterate over the entire training data.\n",
    "- batch_size: This argument specifies the number of samples per gradient update. ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da331ab-bc4d-4bea-9369-02af441a46a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eef9d1c-612a-4709-bdab-51408992ca81",
   "metadata": {},
   "source": [
    "The model will be trained on the X_train data with the corresponding y_train labels using the categorical crossentropy loss function and the Adam optimizer for 10 epochs with a batch size of 32. \n",
    "The training process will be evaluated with the accuracy metric."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e78b8c-7c84-44ee-a1d0-84d0c43ce27c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Evaluate the trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e82297e-993f-48d8-9fd9-2bbfd0cb77ff",
   "metadata": {},
   "source": [
    "Evaluate the model on the test data. The evaluate function is used to evaluate the performance of the model on the test data.\n",
    "- X_test and y_test: These arguments specify the test data and labels, respectively. X_test is the input data and y_test is the corresponding target data.\n",
    "- The evaluate() function returns a list of evaluation metrics, which is stored in the variables test_loss and test_acc. \n",
    "- The test_loss variable contains the value of the loss function (e.g. categorical_crossentropy) on the test data, and \n",
    "- the test_acc variable contains the value of the accuracy metric on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7e4658-fa19-4509-9193-2a903b07d583",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb2ae9e-f2a5-4541-9938-3b2851470139",
   "metadata": {},
   "source": [
    "The test accuracy is a measure of how well the model is able to make predictions on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81b626c-c00c-4256-aab7-5dbcf1b6183c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f919698d-d12f-4aa9-9f88-e77bfa097406",
   "metadata": {},
   "source": [
    "It is important to note that the evaluate() function uses the data provided to make predictions and calculate the performance metric, but it doesn't update the model weights. It is used to evaluate the performance of a trained model on new data, to see how well it generalizes to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab20edf-da72-49ad-8b7c-392d155dceb1",
   "metadata": {},
   "source": [
    "Reflection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b3acaf-c0e4-40a2-915c-7cb43b68e0c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
