{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01485e77-4dcd-4196-930a-d748d4dfa097",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc9f33ac-2298-44bc-addc-fc0b862c345c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import os\n",
    "import random\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import boto3\n",
    "from pynwb import NWBHDF5IO\n",
    "import fsspec\n",
    "from fsspec.implementations.cached import CachingFileSystem\n",
    "import requests\n",
    "import aiohttp\n",
    "\n",
    "# import sys\n",
    "# sys.path.append('/Users/konstantinoskalaitzidis/Developer/dmc/my-gitlab-konkalaitzidis/direct-Behavior-prediction-from-miniscope-calcium-imaging-using-convolutional-neural-networks')\n",
    "# import arrowmaze_project.utils.readSessionsServer as readSessionServer\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e82ea39-5b6c-4c12-a725-4e8c43abb055",
   "metadata": {
    "tags": []
   },
   "source": [
    "NOTE: All dependencies are within a conda environment to ensure reproducibility. To install all dependencies: pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d48b523-c985-491b-ad2d-e95d1c529a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect with dmcdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fbc476-e2ba-4685-823f-332c7b8095fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mySession = readSessionServer.SessionIterator()\n",
    "# sess = mySession.findSession()\n",
    "# # for sess in mySession.findSessions():\n",
    "# #     print(sess)\n",
    "# if sess.hasBehavior() and sess.hasCalcium():\n",
    "#     behavior = sess.getBehaviorSegmentation(align_with_calcium=True).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3d1a2e-7307-44d7-9863-dd06baee6227",
   "metadata": {},
   "source": [
    "## Define hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719e2a1c-7040-4335-8231-bb45e2893d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMG_SIZE = 224\n",
    "# BATCH_SIZE = 64\n",
    "# EPOCHS = 10\n",
    "\n",
    "# MAX_SEQ_LENGTH = 20\n",
    "# NUM_FEATURES = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd45347-075a-4435-bddb-288d8bc3e506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.path.append(\"/Users/konstantinoskalaitzidis/Developer/dmc\")\n",
    "# from readSessionsServer import SessionIterator\n",
    "\n",
    "#TODO: Script to retrieve videos from a list of calcium videos (of the same animal) from the db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941e710a-0385-4e70-b22f-2fc22953ff07",
   "metadata": {},
   "source": [
    "## Data preparation and feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1cc62e-96d3-4a65-aaff-a4f869cff76b",
   "metadata": {},
   "source": [
    "Extract frames from the calcium imaging video and save to directory. Each frame contains spatial information, and the sequence of those frames contains temporal information. Maybe also ask for path input from the user to make it reproducible\n",
    "\n",
    "helpful source: https://keras.io/examples/vision/video_classification/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51d84e0-6996-46bb-8d8f-e3e30d7b0250",
   "metadata": {},
   "source": [
    "Not going to be used for now but this will allow us to have an overview of all the videos I have available to train my CNN model. I expect to have all recordings sessions for each animal as a CNN is going to be trained only based on recordings from the corresponding animal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c8f7e6-4142-4d8c-abde-d6aeedf14113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = pd.read_csv(\"train.csv\")\n",
    "# test_df = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# print(f\"Total videos for training: {len(train_df)}\")\n",
    "# print(f\"Total videos for testing: {len(test_df)}\")\n",
    "\n",
    "# train_df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7dcdb7b-7022-485c-9c96-591b3711feab",
   "metadata": {},
   "source": [
    "The duration of each frame depends on the frame rate of the video. If a video has a frame rate of 25 fps, then each frame will have a duration of 1/25th of a second, or approximately 0.04 seconds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580b2498-d35a-4f44-a6a7-bc2804ac9af5",
   "metadata": {},
   "source": [
    "-> Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a136ef5-f526-417f-bc03-855d4e07fb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch all calcium videos from the dmc database and align calcium videos with behavior annotations\n",
    "# mySession = readSessionServer.SessionIterator()\n",
    "# for sess in mySession.findSessions():\n",
    "#     print(sess)\n",
    "    # if sess.hasBehavior() and sess.hasCalcium():\n",
    "        # behavior = sess.getBehaviorSegmentation(align_with_calcium=True).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d03568-3a6e-41db-9156-26c53d86c0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw calcium imaging video path of animal 1 learning day 1\n",
    "#For accessing the file in a folder contained in the current folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c7bceb-62a8-4589-874f-bd05dd118bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the HDF5 file\n",
    "# with h5py.File('/Users/konstantinoskalaitzidis/Developer/dmc/thesis_data/20211016_163921_animal1learnday1.nwb', 'r') as f:\n",
    "#     # Print the keys of the file\n",
    "#     print(list(f.keys()))\n",
    "#     # dataset = f['identifier'][()]\n",
    "#     # print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e9d178-da84-4268-a626-e8ae346556ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where frames from video will be stored after extraction\n",
    "# frames_dir = \"/Users/konstantinoskalaitzidis/Developer/dmc/thesis_data/CI_video_frames/video_1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ff7be4-a7cc-44a3-9d1a-f7950e030cba",
   "metadata": {},
   "source": [
    "-> Here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f96f31b-c607-4a50-bb5f-9ef8fecdd3e3",
   "metadata": {},
   "source": [
    "The number of frames may differ from video to video.\n",
    "The frame rate may also differ from video to video but it should be 20fps for all. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbee6769-dd9e-4d13-bcb3-7b51b960dce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the video using OpenCV and count the number of frames\n",
    "# cap = cv2.VideoCapture(raw_calcium_video_path)\n",
    "# frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "# cap.release()\n",
    "\n",
    "# print(f\"Number of frames in the video: {frame_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d569b83b-a30e-4837-a3cd-cee58fe2d60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to open other videos for test and frame rate and number of frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bce9c58-3dd8-4fd6-afb3-9dfb9473b873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# video = '/Users/konstantinoskalaitzidis/Desktop/realshort.mp4'\n",
    "\n",
    "# cap = cv2.VideoCapture(video)\n",
    "# frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "# cap.release()\n",
    "\n",
    "# print(f\"Number of frames in the video: {frame_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fed508-e062-4c0e-8c82-5c49e8c5314d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the video file\n",
    "# cap = cv2.VideoCapture(video)\n",
    "\n",
    "# # Get the frame rate of the video\n",
    "# frame_rate = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "# # Release the video capture object\n",
    "# cap.release()\n",
    "\n",
    "# print(f\"Frame rate of the video: {frame_rate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2146d11a-949c-4877-a84c-5d2e762967d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save each frame as one image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787686cd-d9a4-4939-8ab2-7b7b3f8fbffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cap = cv2.VideoCapture(video)\n",
    "\n",
    "# # Loop through the video frames and save each one as an image file\n",
    "# frame_count = 0\n",
    "# while(cap.isOpened()):\n",
    "#     ret, frame = cap.read()\n",
    "#     if ret == False:\n",
    "#         break\n",
    "#     # Save the frame as an image file\n",
    "#     frame_file = os.path.join(frames_dir, \"frame_\" + str(frame_count) + \".jpg\")\n",
    "#     cv2.imwrite(frame_file, frame)\n",
    "#     frame_count += 1\n",
    "\n",
    "# # Close the video file\n",
    "# cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6d055e-0c52-4bcf-80f6-00594de77c4c",
   "metadata": {},
   "source": [
    "### Align behavior annotation with calcium video frames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0f7437-67b9-4386-ae8a-e6bff4e0dc7f",
   "metadata": {},
   "source": [
    "At some point I will also have to align behavior and calcium imaging file and use those as input instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0935a7ed-c016-43fe-9c33-a09c381e2995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # define paths\n",
    "# video_path = '/Users/konstantinoskalaitzidis/Developer/dmc/thesis_data/20211016_163921_animal1learnday1.nwb'\n",
    "# train_dir = '/Users/konstantinoskalaitzidis/Developer/dmc/thesis_data/train'\n",
    "# test_dir = '/Users/konstantinoskalaitzidis/Developer/dmc/thesis_data/test'\n",
    "\n",
    "# # define train-test split ratio\n",
    "# train_test_ratio = 0.8\n",
    "\n",
    "# # open video file\n",
    "# cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# # get video frame count\n",
    "# frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "# # create list of frame indices\n",
    "# frame_indices = list(range(frame_count))\n",
    "\n",
    "# # shuffle frame indices\n",
    "# random.shuffle(frame_indices)\n",
    "\n",
    "# # split frame indices into train and test sets\n",
    "# train_frame_indices = frame_indices[:int(frame_count * train_test_ratio)]\n",
    "# test_frame_indices = frame_indices[int(frame_count * train_test_ratio):]\n",
    "\n",
    "# # iterate over frames and save to train or test directory\n",
    "# for i in range(frame_count):\n",
    "#     # read frame\n",
    "#     ret, frame = cap.read()\n",
    "#     if not ret:\n",
    "#         break\n",
    "    \n",
    "#     # save frame to train or test directory\n",
    "#     if i in train_frame_indices:\n",
    "#         cv2.imwrite(os.path.join(train_dir, f'{i}.jpg'), frame)\n",
    "#     else:\n",
    "#         cv2.imwrite(os.path.join(test_dir, f'{i}.jpg'), frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c706fe-8097-4226-b82f-71c07db34c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# s3_client = boto3.client(\"s3\", endpoint_url=\"https://s3.ki.se\")\n",
    "\n",
    "# def generate_s3_url(path, expires_in=15*60):\n",
    "#     split = re.search(\"https://s3.ki.se/([^/]+)/(.+)\", path)\n",
    "#     if split is None:\n",
    "#         raise ValueError(f\"Path {path} not a valid object under https://s3.ki.se/\")\n",
    "#     params = dict(Bucket=split[1], Key=split[2])\n",
    "#     return s3_client.generate_presigned_url('get_object',\n",
    "#                                             Params=params, \n",
    "#                                             ExpiresIn=expires_in)\n",
    "\n",
    "\n",
    "# Replace \"my-bucket\" and \"my-key\" with the appropriate values for your S3 object\n",
    "# bucket_name = \"my-bucket\"\n",
    "# key = \"my-key\"\n",
    "\n",
    "# # Construct the S3 URL for the object\n",
    "# s3_url = f\"s3://{bucket_name}/{key}\"\n",
    "\n",
    "# # Use Smart Open to retrieve the contents of the S3 object\n",
    "# with smart_open.open(s3_url, \"rb\") as f:\n",
    "#     data = f.read()\n",
    "\n",
    "# # The contents of the S3 object are now stored in the \"data\" variable\n",
    "# print(data)\n",
    "\n",
    "# endpoint_url = \"https://s3.ki.se\"\n",
    "# bucketname = 'dmc-pfcneuropixels'\n",
    "# session = boto3.session.Session()\n",
    "# client = session.client(service_name='s3',endpoint_url=endpoint_url)#,a\n",
    "# #listing all objects, here you could filter out what you want, eg if key == my/path/within/bucket\n",
    "# for myobj in B.objects.all():\n",
    "#     print(myobj)\n",
    "#     #e.g: s3.ObjectSummary(bucket_name='dmc-pfcneuropixels', key='raw-data/passive/PL034_2019-06-05/PL034_g0_t0.imec0.lf.meta')\n",
    "# body = myobj.get()['Body']\n",
    "# body.read() #--> reads the thing, maybe ;)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08e7ec8-dc26-4211-82a6-0fbb4f7277fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = CachingFileSystem(\n",
    "    fs=fsspec.filesystem(\"http\"),\n",
    "    cache_storage=\"nwb-cache\",  # Local folder for the cache\n",
    ")\n",
    "\n",
    "with fs.open('https://s3.ki.se/dmc-striatum-arrowmaze/processed-data/miniscope-recordings/export-to-nwb/animal1learnday1/20211016_163921_animal1learnday1.nwb?AWSAccessKeyId=5AMYRX4EUZ0MV0276K24&Signature=rfGsXZ0Y1nJUrQViB2%2B%2FqoNwfI8%3D&Expires=1679414608', \"rb\") as f:\n",
    "    with h5py.File(f) as file:\n",
    "        video_data = np.array(file[\"analysis/recording_20211016_163921-PP-BP-MC/data\"])\n",
    "\n",
    "# s3_url = 'https://s3.ki.se/dmc-striatum-arrowmaze/processed-data/miniscope-recordings/export-to-nwb/animal1learnday1/20211016_163921_animal1learnday1.nwb'\n",
    "# with NWBHDF5IO(s3_url, mode='r', load_namespaces=True, driver='ros3') as io:\n",
    "#     nwbfile = io.read()\n",
    "#     print(nwbfile)\n",
    "#     print(nwbfile.acquisition['lick_times'].time_series['lick_left_times'].data[:])\n",
    "\n",
    "\n",
    "# s3_credentials = boto3.Session().get_credentials().get_frozen_credentials()\n",
    "# url = 'https://s3.ki.se/dmc-striatum-arrowmaze/processed-data/miniscope-recordings/export-to-nwb/animal1learnday1/20211016_163921_animal1learnday1.nwb'\n",
    "# h5py.File(url, driver=\"ros3\", secret_id=bytes(s3_credentials.access_key, \"utf-8\"), secret_key=bytes(s3_credentials.secret_key, \"utf-8\"), aws_region=b\"us-east-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862dca4f-8d4b-40c0-b6d6-75fc5ef22a53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "video_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96492356-9060-49fe-8d23-a26ffab0d66a",
   "metadata": {},
   "source": [
    "Loading Calcium Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859fb685-6cca-44a6-9a90-de92b7ef9476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data from the .nwb file\n",
    "# with h5py.File('/Users/konstantinoskalaitzidis/Developer/dmc/thesis_data/20211016_163921_animal1learnday1.nwb', 'r') as f:\n",
    "#     video_data = np.array(f['analysis/recording_20211016_163921-PP-BP-MC/data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb5e2e43-31a0-454a-8264-5227aef46851",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'video_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Determine the size of the dataset\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m num_samples \u001b[38;5;241m=\u001b[39m \u001b[43mvideo_data\u001b[49m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      3\u001b[0m num_samples\n",
      "\u001b[0;31mNameError\u001b[0m: name 'video_data' is not defined"
     ]
    }
   ],
   "source": [
    "# Determine the size of the dataset\n",
    "num_samples = video_data.shape[0]\n",
    "num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92022b2e-86f0-4aa4-b829-1a408f0c2daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbce3bf4-2550-4fde-b963-f49a293cdd96",
   "metadata": {},
   "source": [
    "Loading the behavior segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66553c6-20f8-40b6-a5e7-4726a68ff0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with h5py.File('/Users/konstantinoskalaitzidis/Developer/dmc/thesis_data/20211016_163921_animal1learnday1.h5', 'r') as f:\n",
    "#     print(list(f.keys()))\n",
    "#     behavior_data = np.array(f['per_frame'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d54f82-d132-4275-b2c4-d10cd2f3cf75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ab23843d-4105-4cab-ad98-093127d38875",
   "metadata": {},
   "source": [
    "Loading the bonsai data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0e8e73-b364-41ba-bc6c-77297c1c7bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing bonsai data file\n",
    "bonsai_data_path = input(\"Insert path of behavioral data file here: \")\n",
    "bonsai_data = pd.read_csv(bonsai_data_path, header=None)\n",
    "\n",
    "\n",
    "# Adding column names\n",
    "bonsai_data = bonsai_data.rename(columns={\n",
    "    0: 'Time', 1: 'Trial_Number',\n",
    "    2: 'Reward', 3: 'Frame_Number', 4: 'Central_Zone',\n",
    "    5: 'L_Zone', 6: 'R_Zone', 7: 'Calcium_frame'})\n",
    "\n",
    "bonsai_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1094bc91-3973-4343-990b-bf86017748c1",
   "metadata": {},
   "source": [
    "Loading the behavior segmentation file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673ba347-f60b-45b1-8688-95223692988d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_behavior = pd.read_hdf('/Users/konstantinoskalaitzidis/Developer/dmc/thesis_data/20211016_163921_animal1learnday1.h5', 'per_frame')\n",
    "df_behavior.head()\n",
    "\n",
    "# align the calcium frame column from the bonsai file with the behavior file\n",
    "df_behavior = df_behavior.loc[bonsai_data.groupby('Calcium_frame').first()[1:].Frame_Number].reset_index()\n",
    "df_behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84044b8-b3fb-428f-8c1d-b7ca069bbe01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, for each calcium video frame, I want to give the state_id value annotation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca7751e-084b-4508-86e5-31fc09446830",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = video_data\n",
    "train_labels = df_behavior['state_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3c17b9-50bb-400d-b615-28aaa079342d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first image in the training set\n",
    "plt.imshow(train_images[0], cmap=plt.cm.binary)\n",
    "plt.show()\n",
    "\n",
    "# Print the label for the first image in the training set\n",
    "print(train_labels[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6f12d4-99d1-4d15-b87a-ea878ccc710e",
   "metadata": {},
   "source": [
    "We have 24186 images of dimensions 349x374 and the number 1 demonstrates that images are grayscale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e7f6c0-9230-4e53-93d2-326945be9434",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = video_data.reshape(24186, 349, 374, 1)\n",
    "#train_labels = df_behavior['state_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ada29c-4250-4944-b23d-7617fc8c3813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensuring that the pixel values are float numbers. This is a common preprocessing step for image data\n",
    "train_images = train_images.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683b6765-788b-4c8e-9297-f767f21343e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many distinct behaviors do we have?\n",
    "no_of_behaviors = len(df_behavior['state_id'].unique())\n",
    "no_of_behaviors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab19e95e-35c5-4527-9836-2e2e5d93e8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.array([3, 2, 0, 1])\n",
    "\n",
    "print(np.sort(arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e719323-0019-4178-bcb9-c611ab2dd35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_behaviors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9cdf33d-890a-48d1-b2f9-4c733dfaec97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5523f223-0b33-415c-9741-7087ab8b8dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting labels to categorical.\n",
    "train_labels = to_categorical(train_labels, no_of_behaviors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc96b2f4-45ee-4f04-a98d-910c9d5f2cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define image dimensions and number of classes\n",
    "img_height = 349\n",
    "img_width = 374\n",
    "num_classes = 26"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3765499e-836e-4262-bb36-7ecdea38fb4f",
   "metadata": {},
   "source": [
    "## Build the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6c8703-96b4-479e-9861-f57f5d97f662",
   "metadata": {},
   "source": [
    "Creating a sequential model.\n",
    "A sequential model is a linear stack of layers, where the output of one layer is the input of the next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa804f4-3f2e-49bd-9e1d-7da0de67c256",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548c3341-2b1b-42c1-ac33-6fa0c14429da",
   "metadata": {},
   "source": [
    "Add a convolutional layer with 32 filters, a kernel size of 3x3, and a ReLU activation function\n",
    "The ReLU activation function is a simple equation that takes the input of a neuron and returns the input if it is positive, and returns 0 if it is negative. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0e5732-bac4-4710-99c9-0eed2a4622ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(img_height, img_width, 1))) # input is a 28x28 image with 1 color channel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5baf5a-38d0-4a7b-a3c6-c7cb51540013",
   "metadata": {},
   "source": [
    "Add a max pooling layer with a pool size of 2x2\n",
    "This layer applies a max operation over a 2x2 window of the input, reducing the spatial dimensions of the input by half."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0fc581-7b4c-4d74-912e-cd63e1daebb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Add a convolutional layer with 64 filters, a kernel size of 3x3, and a ReLU activation function\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "\n",
    "# Add a max pooling layer with a pool size of 2x2\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Flatten the output from the previous layers\n",
    "model.add(Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ce33f6-1cb5-4487-8322-c1a0b341d4d8",
   "metadata": {},
   "source": [
    "Add a fully connected layer with 128 units and a ReLU activation function. This layer has 128 neurons and it is fully connected to the previous layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31514674-296d-4741-b9f7-141f64a81861",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(128, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a7aac7-1e65-4f9a-a0e0-d42b78758aa6",
   "metadata": {},
   "source": [
    "Add a final output layer with 26 units and a softmax activation function\n",
    "The softmax function is used to convert the output of the final layer into probability distribution over 10 possible classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c50fcf6-9d1b-4ad0-817e-3fc73a360c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ef17f8-dc33-4276-8a8b-54251098954b",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01731528-e4cc-44e2-af01-09a1708ba9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a729faf3-c82c-410b-aaa7-a819f0595fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_images, train_labels, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c7f0d1-8e62-4c31-8d65-d7e144301753",
   "metadata": {},
   "source": [
    "1. Insufficient data? One calcium video of 24186 frames and with 349x374 dimensions.\n",
    "2. Model architecture not appropriate. Try increasing the number of layers or filters, or adding more complex layers like BatchNormalization, Dropout, or Conv2DTranspose.\n",
    "3. Incorrect data preprocessing\n",
    "4. Incorrect hyperparameters\n",
    "5. Class Imbalance (do oversampling, or undersampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04838f8-9caf-4f49-8da4-c14c662e6e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Define image dimensions and number of classes\n",
    "img_height = 349\n",
    "img_width = 374\n",
    "num_classes = 26\n",
    "\n",
    "# Define data generators with data augmentation and normalization\n",
    "#train_datagen = ImageDataGenerator(rescale=1./255, rotation_range=10, width_shift_range=0.1, height_shift_range=0.1)\n",
    "#valid_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,          # Rescale pixel values from [0, 255] to [0, 1]\n",
    "    shear_range=0.2,         # Apply shear transformation to input images\n",
    "    zoom_range=0.2,          # Apply random zoom to input images\n",
    "    horizontal_flip=True,    # Flip images horizontally\n",
    "    validation_split=0.2     # Use 20% of training data as validation data\n",
    ")\n",
    "\n",
    "img_height, img_width = 349, 374\n",
    "batch_size = 32\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    '/Users/konstantinoskalaitzidis/Developer/dmc/thesis_data/train',\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='training'        # Use only the training data\n",
    ")\n",
    "\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory('/Users/konstantinoskalaitzidis/Developer/dmc/thesis_data/20211016_163921_animal1learnday1.nwb', target_size=(img_height, img_width), batch_size=32, class_mode='categorical')\n",
    "#valid_generator = valid_datagen.flow_from_directory('/path/to/valid', target_size=(img_height, img_width), batch_size=32, class_mode='categorical')\n",
    "\n",
    "# Define model architecture\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(img_height, img_width, 3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Compile model with custom optimizer and learning rate\n",
    "opt = Adam(lr=0.001)\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Use early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Train model with data generators and early stopping\n",
    "history = model.fit(train_generator, epochs=30, validation_data=valid_generator, callbacks=[early_stopping])\n",
    "\n",
    "# # Evaluate model on test data\n",
    "# test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "# test_generator = test_datagen.flow_from_directory('/path/to/test', target_size=(img_height, img_width), batch_size=32, class_mode='categorical')\n",
    "# loss, accuracy = model.evaluate(test_generator)\n",
    "\n",
    "# print('Test loss:', loss)\n",
    "# print('Test accuracy:', accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b192bf-a682-48c0-bcf4-f30847b0da8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ae0843-9623-4e7f-8e88-a647d9406d63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b0684b-764a-418c-a033-ff5fe9ce130b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b34b51-5633-42e3-8399-6842489b6bfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d432aa-54b7-4ede-a32d-484c0cf36629",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748adf94-8a85-40de-be8e-74cb09a65d6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b4535b-76aa-495b-af5c-a336c6041b5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08cf954-8259-4ce6-80f4-54f8eacd4bfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dce2c15-c2e3-4e26-909f-094cb2312873",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1916c4c2-ccea-429e-979b-d578e35abc80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e264baed-060d-49a3-a683-c20119c0e717",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a17e0be-a13d-49cd-a59f-20e9d506eb73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055fac62-836b-4bf0-902d-c1da5fd06259",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f26c293-9342-469a-9349-e66712e00d3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72459b1e-89f4-4e16-9669-0a83bff693a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccd5ccf-ce92-495a-8c33-c31b8769942a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee0aba4-e151-48a9-8e46-eefb5a6da5df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b24c5e0-d49b-497c-ae5b-adb173be8be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Define the CNN architecture\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.MaxPooling2D((2,2)),\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2,2)),\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Load the training data\n",
    "(train_images, train_labels), _ = tf.keras.datasets.mnist.load_data()\n",
    "train_images = train_images.reshape((60000, 28, 28, 1))\n",
    "train_images = train_images / 255.0\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_images, train_labels, epochs=5, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1831855-cb2a-4360-97d2-d93a9c782d88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ebd6a5-352b-49d2-a440-18c23c97ecdf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9150a084-d29f-454f-bc30-7cde136024c6",
   "metadata": {},
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c314d14b-a977-4fc0-a03c-aca1cf2e22b3",
   "metadata": {},
   "source": [
    "Don't test. Just train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0066daed-9640-4225-a0c4-529056dd2f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test = train_test_split(video_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Optional: If you have corresponding labels for the data, split them as well\n",
    "with h5py.File('path/to/labels.nwb', 'r') as f:\n",
    "    labels = np.array(f['labels'])\n",
    "\n",
    "y_train, y_test = train_test_split(labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c68976-232e-4c2a-8923-4d4b70c2e95a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58d1515-2404-4ca8-a6aa-724b4577aa13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e446b7a-3c8f-409a-bcfd-150b747daa45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22276208-f41e-4589-9672-50de6e38d4b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05dc765c-32f7-4a47-bc62-518a371b26b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
