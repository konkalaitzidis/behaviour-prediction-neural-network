{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01485e77-4dcd-4196-930a-d748d4dfa097",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e82ea39-5b6c-4c12-a725-4e8c43abb055",
   "metadata": {
    "tags": []
   },
   "source": [
    "NOTE: All dependencies are within a conda environment to ensure reproducibility. To install all dependencies: pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eba8283c-185b-4595-82ad-2e1ac5ec5d97",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-06 14:24:38.433233: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-06 14:24:38.471706: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-06 14:24:39.282282: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-06 14:24:39.295721: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-06 14:24:39.295832: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "# #Lets see if tensorflow finds the GPU\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "#import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4384096d-cb3e-4056-b096-8fa2252c12b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-06 14:24:39.299052: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-06 14:24:39.299640: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-06 14:24:39.299742: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-06 14:24:39.299811: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-06 14:24:39.602749: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-06 14:24:39.602862: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-06 14:24:39.602933: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-06 14:24:39.602991: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21747 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=float32, numpy=array([2.], dtype=float32)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets see if it works\n",
    "tf.ones(1) + tf.ones(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc9f33ac-2298-44bc-addc-fc0b862c345c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np # for working with arrays and matrices\n",
    "import pandas as pd # for data manipulation and analysis\n",
    "import matplotlib.pyplot as plt # for data visualization\n",
    "import seaborn as sns # for data visualization\n",
    "import time # for time-related functions\n",
    "import random # for random number generation\n",
    "import cv2 # for computer vision and image processing tasks\n",
    "import datetime # for saving date and time information\n",
    "\n",
    "\n",
    "\n",
    "import h5py # for working with HDF5 (Hierarchical Data Format) files\n",
    "import boto3 # for working with Amazon Web Services (AWS)\n",
    "from pynwb import NWBHDF5IO # for working with Neurodata Without Border (NWB) files\n",
    "import fsspec \n",
    "from fsspec.implementations.cached import CachingFileSystem # library used for working with various file systems in Python.\n",
    "import requests \n",
    "import aiohttp # libraries which are used for making HTTP requests in Python.\n",
    "import os # OS module provides various operating system-related functions to the code\n",
    "import csv # CSV module is used for working with CSV (Comma Separated Values) files in Python.\n",
    "import pickle\n",
    "\n",
    "\n",
    "# used for splitting data into training and testing sets in Python.\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "# for generating a confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "# Classes and functions from the Keras library which is used for building and training deep learning models in Python.\n",
    "from keras.models import load_model\n",
    "from keras.models import model_from_json\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "# These import the Adam optimizer class and various other classes from the TensorFlow Keras library \n",
    "# which is a high-level neural networks API used for building and training deep learning models in Python.\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3d1a2e-7307-44d7-9863-dd06baee6227",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## [Ignore for now] Define hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "719e2a1c-7040-4335-8231-bb45e2893d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMG_SIZE = 224\n",
    "# BATCH_SIZE = 64\n",
    "# EPOCHS = 10\n",
    "\n",
    "# MAX_SEQ_LENGTH = 20\n",
    "# NUM_FEATURES = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bd45347-075a-4435-bddb-288d8bc3e506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.path.append(\"/Users/konstantinoskalaitzidis/Developer/dmc\")\n",
    "# from readSessionsServer import SessionIterator\n",
    "\n",
    "#TODO: Script to retrieve videos from a list of calcium videos (of the same animal) from the db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941e710a-0385-4e70-b22f-2fc22953ff07",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Dataset preparation and label annotation (feature engineering)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b3d25e-b6b2-423b-9660-33db62493079",
   "metadata": {
    "tags": []
   },
   "source": [
    "### [Ignore for now] Data availale for processing - overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51d84e0-6996-46bb-8d8f-e3e30d7b0250",
   "metadata": {},
   "source": [
    "The following is not going to be used for now but will allow us to have an overview of all the videos I have available to train my CNN model. I expect to have all recordings sessions for each animal as input for the CNN which is going to be trained only based on recordings from the corresponding animal. The data will be split to train/test at some point..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5c8f7e6-4142-4d8c-abde-d6aeedf14113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = pd.read_csv(\"train.csv\")\n",
    "# test_df = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# print(f\"Total videos for training: {len(train_df)}\")\n",
    "# print(f\"Total videos for testing: {len(test_df)}\")\n",
    "\n",
    "# train_df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1cc62e-96d3-4a65-aaff-a4f869cff76b",
   "metadata": {},
   "source": [
    "Extract frames from the calcium imaging video and save to directory. Each frame contains spatial information, and the sequence of those frames contains temporal information (the latter is not exploited for now). Maybe also ask for path input from the user to make it reproducible for others.\n",
    "\n",
    "Helpful source: https://keras.io/examples/vision/video_classification/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f96f31b-c607-4a50-bb5f-9ef8fecdd3e3",
   "metadata": {},
   "source": [
    "The number of frames may differ from video to video.\n",
    "The frame rate may also differ from video to video but it should be 20fps for all. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7dcdb7b-7022-485c-9c96-591b3711feab",
   "metadata": {},
   "source": [
    "The duration of each frame depends on the frame rate of the video. If a video has a frame rate of 25 fps, then each frame will have a duration of 1/25th of a second, or approximately 0.04 seconds. The calcium videos use 20fps, while the behavioral recordings are at 60fps. Alignment of these videos will follow shortly. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a3ae92-9853-43c3-bbef-ede04ab1aec4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### [Ignore for now] Fetch all calcium videos from the dmc database and align calcium videos with behavior annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a136ef5-f526-417f-bc03-855d4e07fb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mySession = readSessionServer.SessionIterator()\n",
    "# for sess in mySession.findSessions():\n",
    "#     print(sess)\n",
    "    # if sess.hasBehavior() and sess.hasCalcium():\n",
    "        # behavior = sess.getBehaviorSegmentation(align_with_calcium=True).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5de1e4-345a-4132-95cc-948a88666259",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### [Ignore for now] Open calcium video locally, create dir for saving frames and count number of frames with OpenCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66c7bceb-62a8-4589-874f-bd05dd118bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the HDF5 file\n",
    "# with h5py.File('/Users/konstantinoskalaitzidis/Developer/dmc/thesis_data/20211016_163921_animal1learnday1.nwb', 'r') as f:\n",
    "#     # Print the keys of the file\n",
    "#     print(list(f.keys()))\n",
    "#     # dataset = f['identifier'][()]\n",
    "#     # print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71e9d178-da84-4268-a626-e8ae346556ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where frames from video will be stored after extraction\n",
    "# frames_dir = \"path\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fbee6769-dd9e-4d13-bcb3-7b51b960dce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the video using OpenCV and count the number of frames\n",
    "# cap = cv2.VideoCapture(raw_calcium_video_path)\n",
    "# frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "# cap.release()\n",
    "\n",
    "# print(f\"Number of frames in the video: {frame_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4bce9c58-3dd8-4fd6-afb3-9dfb9473b873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# video = 'path'\n",
    "\n",
    "# cap = cv2.VideoCapture(video)\n",
    "# frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "# cap.release()\n",
    "\n",
    "# print(f\"Number of frames in the video: {frame_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98fed508-e062-4c0e-8c82-5c49e8c5314d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the video file\n",
    "# cap = cv2.VideoCapture(video)\n",
    "\n",
    "# # Get the frame rate of the video\n",
    "# frame_rate = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "# # Release the video capture object\n",
    "# cap.release()\n",
    "\n",
    "# print(f\"Frame rate of the video: {frame_rate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2146d11a-949c-4877-a84c-5d2e762967d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save each frame as one image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "787686cd-d9a4-4939-8ab2-7b7b3f8fbffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cap = cv2.VideoCapture(video)\n",
    "\n",
    "# # Loop through the video frames and save each one as an image file\n",
    "# frame_count = 0\n",
    "# while(cap.isOpened()):\n",
    "#     ret, frame = cap.read()\n",
    "#     if ret == False:\n",
    "#         break\n",
    "#     # Save the frame as an image file\n",
    "#     frame_file = os.path.join(frames_dir, \"frame_\" + str(frame_count) + \".jpg\")\n",
    "#     cv2.imwrite(frame_file, frame)\n",
    "#     frame_count += 1\n",
    "\n",
    "# # Close the video file\n",
    "# cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6d055e-0c52-4bcf-80f6-00594de77c4c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## [Start here] Align behavior annotation with calcium video frames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0f7437-67b9-4386-ae8a-e6bff4e0dc7f",
   "metadata": {},
   "source": [
    "At some point I will also have to align the behavior and the calcium imaging videos and use those as input for my CNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96492356-9060-49fe-8d23-a26ffab0d66a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### [Start here] Loading calcium video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c4925dc-953b-46df-adfe-1d6f9001077b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3_calcium_url = 'https://s3.ki.se/dmc-striatum-arrowmaze/processed-data/miniscope-recordings/export-to-nwb/animal3learnday11/20211028_181307_animal3learnday11.nwb?AWSAccessKeyId=5AMYRX4EUZ0MV0276K24&Signature=PebSFYmFWtEYsZdQG5I0YkOLdu4%3D&Expires=1680773491'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e378c962-5c93-408d-8d3b-dbf38dff6b50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'animal_3_learning_day_11'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Animal and learning day:\n",
    "animal_no = 3\n",
    "learning_day = 11\n",
    "video_name = 'animal_'+str(animal_no)+'_learning_day_'+str(learning_day)\n",
    "video_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d08e7ec8-dc26-4211-82a6-0fbb4f7277fc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 0 hours, 0 minutes, 3 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "\n",
    "fs = CachingFileSystem(\n",
    "    fs=fsspec.filesystem(\"http\"),\n",
    "    cache_storage=\"nwb-cache\",  # Local folder for the cache\n",
    ")\n",
    "\n",
    "with fs.open(s3_calcium_url, \"rb\") as f:\n",
    "    with h5py.File(f) as file:\n",
    "        video_data = np.array(file[\"analysis/recording_20211028_181307-PP-BP-MC/data\"])\n",
    "        \n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "hours, remainder = divmod(execution_time, 3600)\n",
    "minutes, seconds = divmod(remainder, 60)\n",
    "\n",
    "print(f\"Execution time: {int(hours)} hours, {int(minutes)} minutes, {int(seconds)} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b06d00bc-8a71-48f7-85f1-1737fa787195",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[175, 181, 182, ..., 181, 181, 180],\n",
       "        [170, 174, 173, ..., 177, 176, 179],\n",
       "        [175, 175, 168, ..., 180, 176, 178],\n",
       "        ...,\n",
       "        [180, 182, 175, ..., 170, 173, 167],\n",
       "        [177, 188, 181, ..., 177, 179, 180],\n",
       "        [184, 184, 183, ..., 188, 182, 174]],\n",
       "\n",
       "       [[183, 185, 178, ..., 176, 175, 180],\n",
       "        [178, 175, 177, ..., 177, 180, 181],\n",
       "        [186, 178, 178, ..., 178, 180, 183],\n",
       "        ...,\n",
       "        [179, 180, 177, ..., 170, 173, 174],\n",
       "        [177, 179, 180, ..., 179, 174, 173],\n",
       "        [177, 186, 185, ..., 186, 180, 178]],\n",
       "\n",
       "       [[179, 181, 180, ..., 182, 184, 179],\n",
       "        [177, 176, 181, ..., 176, 172, 173],\n",
       "        [177, 181, 185, ..., 169, 172, 175],\n",
       "        ...,\n",
       "        [185, 187, 184, ..., 174, 171, 172],\n",
       "        [187, 191, 187, ..., 179, 176, 171],\n",
       "        [188, 184, 184, ..., 185, 182, 175]]], dtype=int16)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_data[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58eab63-b867-44b7-bf80-12c150b4edf7",
   "metadata": {},
   "source": [
    "### Determine the size of the calcium video dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cb5e2e43-31a0-454a-8264-5227aef46851",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of video frames is  24589  and the frame dimensions (height x width) are:  393 X 444\n"
     ]
    }
   ],
   "source": [
    "num_of_frames = video_data.shape[0]\n",
    "img_height = video_data.shape[1]\n",
    "img_width = video_data.shape[2]\n",
    "print(\"The number of video frames is \", num_of_frames, \" and the frame dimensions (height x width) are: \", img_height, \"X\", img_width)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb68ecee-c60b-4bed-abba-b468c813acd5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Normalize pixel values in calcium video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a7456582-732a-4959-97ed-d8c9a65a6428",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum pixel value: 0.000\n",
      "Maximum pixel value: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Normalize pixel values to be between 0 and 1\n",
    "max_pixel_value = video_data.max()\n",
    "min_pixel_value = video_data.min()\n",
    "range_pixel_value = max_pixel_value - min_pixel_value\n",
    "normalized_video_data = (video_data - min_pixel_value) / range_pixel_value\n",
    "video_data = normalized_video_data\n",
    "\n",
    "# Verify the normalization by checking the minimum and maximum values\n",
    "print('Minimum pixel value: {:.3f}' .format(np.min(video_data)))\n",
    "print('Maximum pixel value:', np.max(video_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab23843d-4105-4cab-ad98-093127d38875",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Loading bonsai data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0e0e8e73-b364-41ba-bc6c-77297c1c7bda",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>Trial_Number</th>\n",
       "      <th>Reward</th>\n",
       "      <th>Frame_Number</th>\n",
       "      <th>Central_Zone</th>\n",
       "      <th>L_Zone</th>\n",
       "      <th>R_Zone</th>\n",
       "      <th>Calcium_frame</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-10-28T18:13:25.3758464+02:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>66</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-10-28T18:13:25.3759488+02:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>66</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-10-28T18:13:25.3759872+02:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>66</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-10-28T18:13:25.3851136+02:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>67</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-10-28T18:13:25.4012672+02:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>68</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Time  Trial_Number  Reward  Frame_Number  \\\n",
       "0  2021-10-28T18:13:25.3758464+02:00             0       0            66   \n",
       "1  2021-10-28T18:13:25.3759488+02:00             0       0            66   \n",
       "2  2021-10-28T18:13:25.3759872+02:00             0       0            66   \n",
       "3  2021-10-28T18:13:25.3851136+02:00             0       0            67   \n",
       "4  2021-10-28T18:13:25.4012672+02:00             0       0            68   \n",
       "\n",
       "   Central_Zone  L_Zone  R_Zone  Calcium_frame  \n",
       "0         False   False   False              0  \n",
       "1         False   False   False              0  \n",
       "2         False   False   False              0  \n",
       "3         False   False   False              0  \n",
       "4         False   False   False              0  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preparing bonsai data file.\n",
    "# CSV with additional data from the behavior box, such as reward deliveries. Also includes information needed for synchronizing the calcium and behavioral recordings.\n",
    "bonsai_data_path = '/home/dmc/Desktop/kostas/direct-Behavior-prediction-from-miniscope-calcium-imaging-using-convolutional-neural-networks/data/tmaze_2021-10-28T18_13_23.csv'\n",
    "bonsai_data = pd.read_csv(bonsai_data_path, header=None)\n",
    "\n",
    "\n",
    "# Adding column names\n",
    "bonsai_data = bonsai_data.rename(columns={\n",
    "    0: 'Time', 1: 'Trial_Number',\n",
    "    2: 'Reward', 3: 'Frame_Number', 4: 'Central_Zone',\n",
    "    5: 'L_Zone', 6: 'R_Zone', 7: 'Calcium_frame'})\n",
    "\n",
    "bonsai_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1094bc91-3973-4343-990b-bf86017748c1",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Loading behavior segmentation file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ed727e01-5b0b-4510-b383-df351fc3187d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frame_no</th>\n",
       "      <th>segment_no</th>\n",
       "      <th>state_id</th>\n",
       "      <th>spatial_progress</th>\n",
       "      <th>temporal_progress</th>\n",
       "      <th>state_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>-0.906529</td>\n",
       "      <td>0.00</td>\n",
       "      <td>leftReturn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>-0.910748</td>\n",
       "      <td>0.02</td>\n",
       "      <td>leftReturn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>-0.916558</td>\n",
       "      <td>0.04</td>\n",
       "      <td>leftReturn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>-0.925125</td>\n",
       "      <td>0.06</td>\n",
       "      <td>leftReturn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>-0.931319</td>\n",
       "      <td>0.08</td>\n",
       "      <td>leftReturn</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   frame_no  segment_no  state_id  spatial_progress  temporal_progress  \\\n",
       "0         0           0        14         -0.906529               0.00   \n",
       "1         1           0        14         -0.910748               0.02   \n",
       "2         2           0        14         -0.916558               0.04   \n",
       "3         3           0        14         -0.925125               0.06   \n",
       "4         4           0        14         -0.931319               0.08   \n",
       "\n",
       "   state_name  \n",
       "0  leftReturn  \n",
       "1  leftReturn  \n",
       "2  leftReturn  \n",
       "3  leftReturn  \n",
       "4  leftReturn  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Segmentation of each frame into one behavior class.\n",
    "df_behavior_path = '/home/dmc/Desktop/kostas/direct-Behavior-prediction-from-miniscope-calcium-imaging-using-convolutional-neural-networks/data/20211028_181307_animal3learnday11.h5'\n",
    "df_behavior = pd.read_hdf(df_behavior_path, 'per_frame')\n",
    "df_behavior.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58daa732-a2ea-47d9-9e53-b10920d2967c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Aligning calcium frame column from the bonsai file with the behavior file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6ca4d04f-2366-4290-80c1-c74408f4d0c6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>frame_no</th>\n",
       "      <th>segment_no</th>\n",
       "      <th>state_id</th>\n",
       "      <th>spatial_progress</th>\n",
       "      <th>temporal_progress</th>\n",
       "      <th>state_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>704</td>\n",
       "      <td>704</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0.389899</td>\n",
       "      <td>0.389899</td>\n",
       "      <td>initReward</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>707</td>\n",
       "      <td>707</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0.395960</td>\n",
       "      <td>0.395960</td>\n",
       "      <td>initReward</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>710</td>\n",
       "      <td>710</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0.402020</td>\n",
       "      <td>0.402020</td>\n",
       "      <td>initReward</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>713</td>\n",
       "      <td>713</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0.408081</td>\n",
       "      <td>0.408081</td>\n",
       "      <td>initReward</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>716</td>\n",
       "      <td>716</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0.414141</td>\n",
       "      <td>0.414141</td>\n",
       "      <td>initReward</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  frame_no  segment_no  state_id  spatial_progress  temporal_progress  \\\n",
       "0    704       704          11         0          0.389899           0.389899   \n",
       "1    707       707          11         0          0.395960           0.395960   \n",
       "2    710       710          11         0          0.402020           0.402020   \n",
       "3    713       713          11         0          0.408081           0.408081   \n",
       "4    716       716          11         0          0.414141           0.414141   \n",
       "\n",
       "   state_name  \n",
       "0  initReward  \n",
       "1  initReward  \n",
       "2  initReward  \n",
       "3  initReward  \n",
       "4  initReward  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_aligned = df_behavior.loc[bonsai_data.groupby('Calcium_frame').first()[1:].Frame_Number].reset_index()\n",
    "df_aligned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48056171-61ba-4d0f-bba0-c5dd5bd85750",
   "metadata": {},
   "source": [
    "### Redoing behavior labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e2d5d287-3cd2-426c-9972-9715bfd0e138",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['initReward', 'initRight', 'mainRunLeft', 'turnMainToLeft',\n",
       "       'leftRun', 'leftReward', 'leftRight', 'leftReturn',\n",
       "       'turnLeftToMain', 'mainReturn', 'initLeft', 'leftLeft', 'rightRun',\n",
       "       'rightReward', 'rightRight', 'rightReturn', 'turnRightToMain',\n",
       "       'mainOther', 'rightLeft', 'leftOther', 'turnMainToRight',\n",
       "       'rightOther', 'turnLeftToRight', 'turnRightToLeft', 'mainRunRight'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_aligned['state_name'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "94751056-1acc-4404-8a85-b6db6e884f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_annotations = df_aligned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7f1f1d51-eaac-4ef9-a31a-e5fc5df9372c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "state_id\n",
       "0          initReward\n",
       "1            initLeft\n",
       "2           initRight\n",
       "3         mainRunLeft\n",
       "4        mainRunRight\n",
       "5          mainReturn\n",
       "6           mainOther\n",
       "7      turnMainToLeft\n",
       "8     turnMainToRight\n",
       "9      turnLeftToMain\n",
       "10    turnRightToMain\n",
       "11    turnLeftToRight\n",
       "12    turnRightToLeft\n",
       "13            leftRun\n",
       "14         leftReturn\n",
       "15         leftReward\n",
       "16           leftLeft\n",
       "17          leftRight\n",
       "18          leftOther\n",
       "19           rightRun\n",
       "20        rightReturn\n",
       "21        rightReward\n",
       "22          rightLeft\n",
       "23         rightRight\n",
       "24         rightOther\n",
       "Name: state_name, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new_annotations[['state_id', 'state_name']]\n",
    "df_unique_states = df_new_annotations[['state_id', 'state_name']].drop_duplicates(subset='state_id')\n",
    "df_unique_states = df_unique_states.set_index('state_id')['state_name']\n",
    "df_unique_states = df_unique_states.sort_index()\n",
    "df_unique_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "61ca7ef7-cf5b-4142-b772-677e03e87d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# state_id_unique = df_new_annotations['state_id'].unique()\n",
    "# state_name_unique = df_aligned['state_name'].unique()\n",
    "# # create a dictionary with the unique values\n",
    "# unique_values_dict = {'state_id': state_id_unique, 'state_name': state_name_unique}\n",
    "# # create a new dataframe from the dictionary\n",
    "# df_unique_values = pd.DataFrame.from_dict(unique_values_dict)\n",
    "# df_unique_values.sort_values(by=['state_id'], ascending=[True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c78a261b-b80c-4f67-a186-12223d558c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main: initReward, initLeft, initRight, mainRunLeft, mainRunRight, mainReturn, mainOther, turnMainToLeft, turnMainToRight\n",
    "# left: turnLeftToMain, turnLeftToRight, leftRun, leftReturn, leftReward, leftLeft, leftRight, leftOther\n",
    "# right: turnRightToMain, turnRightToLeft, rightRun, rightReturn, rightReward, rightLeft, rightRight, rightOther "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0490d715-cea6-4b2b-9066-63109aeda362",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_annotations['state_id'] = df_new_annotations['state_id'].replace({1: 0, 2:0, 3:0, 4:0, 5:0, 6:0, 7:0, 8:0})\n",
    "df_new_annotations['state_id'] = df_new_annotations['state_id'].replace({9:1, 11:1, 13:1, 14:1, 15:1, 16:1, 17:1, 18:1})\n",
    "df_new_annotations['state_id'] = df_new_annotations['state_id'].replace({10:2, 12:2, 19:2, 20:2, 21:2, 22:2, 23:2, 24:2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8483d272-1759-4a7a-ad50-bff3bc29375b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_annotations_unique = df_new_annotations['state_id'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bab359-ef5c-483e-9060-aec8dc9b5f8c",
   "metadata": {},
   "source": [
    "### Verify the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e3154838-b59f-459a-a7a0-537bc07e1004",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_labels = train_labels.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9bcde91b-740e-45ba-8d0c-5b882e5a79c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Let's plot the first 25 images from the training set and display the class name below each image:\n",
    "# plt.figure(figsize=(10,10))\n",
    "# for i in range(25):\n",
    "#     plt.subplot(5,5,i+1)\n",
    "#     plt.xticks([])\n",
    "#     plt.yticks([])\n",
    "#     plt.grid(False)\n",
    "#     plt.imshow(images[i])\n",
    "#     plt.xlabel(labels[i])\n",
    "#     plt.imshow(images[i], cmap=plt.cm.binary, vmin=0, vmax=1)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6f12d4-99d1-4d15-b87a-ea878ccc710e",
   "metadata": {},
   "source": [
    "We have 24186 images of dimensions 349x374 and the number 1 demonstrates that images are grayscale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bca7751e-084b-4508-86e5-31fc09446830",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For each calcium video frame, I want to give the state_id value annotation. \n",
    "channel_dimension = 1\n",
    "images = video_data.reshape(num_of_frames, img_height, img_width, channel_dimension)\n",
    "labels = df_new_annotations['state_id']\n",
    "\n",
    "#train_labels = df_aligned['state_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5e4b17e5-fa99-407d-8792-100aa47e2284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and validation sets\n",
    "train_images, val_images, train_labels, val_labels = train_test_split(images, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e334e3f6-8286-4ccf-ae19-5099f206fb25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#train_images = video_data.reshape(num_of_frames, img_height, img_width, channel_dimension)\n",
    "#train_labels = df_behavior['state_id']\n",
    "\n",
    "# ensuring that the pixel values are float numbers. This is a common preprocessing step for image data\n",
    "train_images = train_images.astype('float32')\n",
    "val_images = val_images.astype('float32')\n",
    "### Finding number of classes and converting labels to categorical values\n",
    "\n",
    "# How many distinct behaviors do we have?\n",
    "no_of_behaviors = df_new_annotations_unique\n",
    "\n",
    "# Define the number of classes\n",
    "num_classes = len(no_of_behaviors)\n",
    "\n",
    "# Converting labels to categorical.\n",
    "train_labels = to_categorical(train_labels, num_classes)\n",
    "val_labels = to_categorical(val_labels, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8eed501b-12f6-4323-9a6e-3fd395481808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    11336\n",
      "1     9643\n",
      "2     3610\n",
      "Name: state_id, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# check class imbalance\n",
    "# count the number of instances of each class\n",
    "class_counts = pd.value_counts(df_new_annotations['state_id'])\n",
    "# print the counts of each class\n",
    "print(class_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5faa215c-2120-4a5b-b787-7ebf02d5db80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24589"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_counts = class_counts[0] + class_counts[1] + class_counts[2]\n",
    "total_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c085ef-830d-4978-b756-3481a9df6084",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1534b5d1-cecc-4156-ad52-4086fba686f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Behavior Main is 46.1%\n",
      "Behavior Left is 39.2%\n",
      "Behavior Right is 14.7%\n"
     ]
    }
   ],
   "source": [
    "# calculate the percentage of each class in the dataset\n",
    "class_percents = pd.value_counts(df_new_annotations['state_id'], normalize=True) * 100\n",
    "\n",
    "# create a bar chart of class percentages\n",
    "plt.bar(class_percents.index, class_percents.values)\n",
    "\n",
    "# add axis labels and a title\n",
    "plt.xlabel('Class Label')\n",
    "plt.ylabel('Percentage of Instances')\n",
    "plt.title('Distribution of Class Labels')\n",
    "\n",
    "# display the plot\n",
    "plt.show()\n",
    "\n",
    "print(\"Behavior Main is {:.1f}%\" .format((class_counts[0]/total_counts)*100))\n",
    "print(\"Behavior Left is {:.1f}%\" .format((class_counts[1]/total_counts)*100))\n",
    "print(\"Behavior Right is {:.1f}%\" .format((class_counts[2]/total_counts)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3765499e-836e-4262-bb36-7ecdea38fb4f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Build the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ef17f8-dc33-4276-8a8b-54251098954b",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c079849b-d1d1-49fe-9769-ca91499236a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Input channel dimension (Greyscale: 1, RGB: 3)\n",
    "#channel_dimension = 1\n",
    "#channel_dimension = int(input(\"Input channel dimension (Greyscale: 1, RGB: 3)\"))\n",
    "#Improve in case the user clicks smth else"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5ddb6718-57e0-4572-a68d-048cabc76fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and compile your CNN model here\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# # Train your model here\n",
    "# model.fit(train_images, train_labels, epochs=10, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c3950500-65a6-465c-80b1-1f197b439fa5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# training parameters\n",
    "epochs = 100\n",
    "batch_size = 32\n",
    "channel_dimension = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5053ace3-7dda-4a21-90ec-937acf44057a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def construct_model(input_shape, num_classes, name):\n",
    "    \n",
    "    # Creating a sequential model. A sequential model is a linear stack of layers, where the output of one layer is the input of the next.\n",
    "    model = Sequential(name=name)\n",
    "\n",
    "    # Add a convolutional layer with 32 filters, a kernel size of 3x3, and a ReLU activation function. \n",
    "    # The ReLU activation function is a simple equation that takes the input of a neuron and returns the input if it is positive, and returns 0 if it is negative.\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape)) # input is a 28x28 image with 1 color channel.\n",
    "\n",
    "    # Add a max pooling layer with a pool size of 2x\n",
    "    2\n",
    "    # This layer applies a max operation over a 2x2 window of the input, reducing the spatial dimensions of the input by half.\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    # Add a convolutional layer with 64 filters, a kernel size of 3x3, and a ReLU activation function\n",
    "    model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "\n",
    "    # Add a max pooling layer with a pool size of 2x2\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    # Flatten the output from the previous layers\n",
    "    model.add(Flatten())\n",
    "\n",
    "    # Add a dropout layer to prevent overfitting\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    # Add a fully connected layer with 128 units and a ReLU activation function. This layer has 128 neurons and it is fully connected to the previous layer\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "\n",
    "    # Add a final output layer with num_classes number of units and a softmax activation function The softmax function is used to convert the output of the final layer into probability distribution over 10 possible classes.\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    # # Complete model \n",
    "    # model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8b742b05-ff61-43b7-ae37-6b41e8f71cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (img_height, img_width, channel_dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "344b1b7e-c6e9-4eaa-b848-4132d926f7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model by calling the function\n",
    "name = 'BPNN_v2'\n",
    "model = construct_model(input_shape, num_classes, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bb1e6242-13d6-425a-9622-52b07971c65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot model architecture\n",
    "#plot_model(model, to_file='model_1.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "15f4243f-94af-4c75-a566-1f988e0dd98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import visualkeras\n",
    "# from PIL import ImageFont\n",
    "# visualkeras.layered_view(model, legend=True)\n",
    "\n",
    "from ann_visualizer.visualize import ann_viz\n",
    "plot = ann_viz(model, view=True, filename=str(name)+\"-architecture\", title=\"CNN — \"+str(name)+\" — Simple Architecture\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3cb5d45b-5356-4310-8cf5-2a0eb143da28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up early stopping criteria\n",
    "# early_stopping = EarlyStopping(monitor='val_loss', patience=3, mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "77a89550-ee3a-4651-b5d7-6a90c5c8a0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_execution():\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.legacy.Adam(), metrics=['accuracy'])    \n",
    "\n",
    "    history = model.fit(train_images, train_labels, epochs=epochs, batch_size=batch_size, validation_data=(val_images, val_labels)) # add early stopping here\n",
    "    \n",
    "    model.save('BPNN_V2_model.h5')\n",
    "\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    hours, remainder = divmod(execution_time, 3600)\n",
    "    minutes, seconds = divmod(remainder, 60)\n",
    "\n",
    "    print(f\"Execution time: {int(hours)} hours, {int(minutes)} minutes, {int(seconds)} seconds\")\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e726aa57-83bd-4e37-8b6f-95c0939e02d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-06 14:25:19.507023: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8401\n",
      "2023-04-06 14:25:20.729819: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "615/615 [==============================] - 29s 41ms/step - loss: 1.0780 - accuracy: 0.4586 - val_loss: 1.0153 - val_accuracy: 0.4516\n",
      "Epoch 2/100\n",
      "615/615 [==============================] - 23s 37ms/step - loss: 1.0098 - accuracy: 0.4576 - val_loss: 0.9989 - val_accuracy: 0.4516\n",
      "Epoch 3/100\n",
      "615/615 [==============================] - 23s 37ms/step - loss: 0.9886 - accuracy: 0.4688 - val_loss: 0.9721 - val_accuracy: 0.5211\n",
      "Epoch 4/100\n",
      "615/615 [==============================] - 23s 37ms/step - loss: 0.9476 - accuracy: 0.5016 - val_loss: 0.9194 - val_accuracy: 0.4929\n",
      "Epoch 5/100\n",
      "615/615 [==============================] - 23s 37ms/step - loss: 0.8871 - accuracy: 0.5605 - val_loss: 0.8283 - val_accuracy: 0.6584\n",
      "Epoch 6/100\n",
      "615/615 [==============================] - 23s 37ms/step - loss: 0.8228 - accuracy: 0.6130 - val_loss: 0.7331 - val_accuracy: 0.6836\n",
      "Epoch 7/100\n",
      "615/615 [==============================] - 23s 37ms/step - loss: 0.7637 - accuracy: 0.6503 - val_loss: 0.7113 - val_accuracy: 0.6879\n",
      "Epoch 8/100\n",
      "615/615 [==============================] - 23s 37ms/step - loss: 0.7291 - accuracy: 0.6738 - val_loss: 0.7073 - val_accuracy: 0.7109\n",
      "Epoch 9/100\n",
      "615/615 [==============================] - 23s 37ms/step - loss: 0.6862 - accuracy: 0.6975 - val_loss: 0.6306 - val_accuracy: 0.7314\n",
      "Epoch 10/100\n",
      "615/615 [==============================] - 23s 37ms/step - loss: 0.6461 - accuracy: 0.7152 - val_loss: 0.6363 - val_accuracy: 0.7222\n",
      "Epoch 11/100\n",
      "615/615 [==============================] - 23s 37ms/step - loss: 0.6187 - accuracy: 0.7322 - val_loss: 0.6032 - val_accuracy: 0.7324\n",
      "Epoch 12/100\n",
      "615/615 [==============================] - 23s 37ms/step - loss: 0.5694 - accuracy: 0.7559 - val_loss: 0.6266 - val_accuracy: 0.7210\n",
      "Epoch 13/100\n",
      "615/615 [==============================] - 23s 37ms/step - loss: 0.5331 - accuracy: 0.7699 - val_loss: 0.5959 - val_accuracy: 0.7310\n",
      "Epoch 14/100\n",
      "615/615 [==============================] - 23s 37ms/step - loss: 0.4947 - accuracy: 0.7931 - val_loss: 0.6157 - val_accuracy: 0.7334\n",
      "Epoch 15/100\n",
      "615/615 [==============================] - 23s 37ms/step - loss: 0.4667 - accuracy: 0.8037 - val_loss: 0.6489 - val_accuracy: 0.7251\n",
      "Epoch 16/100\n",
      "615/615 [==============================] - 23s 37ms/step - loss: 0.4494 - accuracy: 0.8112 - val_loss: 0.5932 - val_accuracy: 0.7371\n",
      "Epoch 17/100\n",
      "615/615 [==============================] - 23s 37ms/step - loss: 0.4087 - accuracy: 0.8295 - val_loss: 0.6928 - val_accuracy: 0.7121\n",
      "Epoch 18/100\n",
      "615/615 [==============================] - 23s 37ms/step - loss: 0.3863 - accuracy: 0.8395 - val_loss: 0.6282 - val_accuracy: 0.7174\n",
      "Epoch 19/100\n",
      "615/615 [==============================] - 23s 37ms/step - loss: 0.3576 - accuracy: 0.8528 - val_loss: 0.6182 - val_accuracy: 0.7389\n",
      "Epoch 20/100\n",
      "615/615 [==============================] - 23s 37ms/step - loss: 0.3278 - accuracy: 0.8663 - val_loss: 0.6983 - val_accuracy: 0.7218\n",
      "Epoch 21/100\n",
      "615/615 [==============================] - 23s 37ms/step - loss: 0.3007 - accuracy: 0.8811 - val_loss: 0.7003 - val_accuracy: 0.7210\n",
      "Epoch 22/100\n",
      "615/615 [==============================] - 23s 37ms/step - loss: 0.2726 - accuracy: 0.8915 - val_loss: 0.7298 - val_accuracy: 0.7214\n",
      "Epoch 23/100\n",
      "615/615 [==============================] - 23s 37ms/step - loss: 0.2625 - accuracy: 0.8951 - val_loss: 0.7186 - val_accuracy: 0.7330\n",
      "Epoch 24/100\n",
      "615/615 [==============================] - 23s 37ms/step - loss: 0.2450 - accuracy: 0.9036 - val_loss: 0.7423 - val_accuracy: 0.7247\n",
      "Epoch 25/100\n",
      "615/615 [==============================] - 23s 37ms/step - loss: 0.2355 - accuracy: 0.9071 - val_loss: 0.8072 - val_accuracy: 0.7182\n",
      "Epoch 26/100\n",
      "615/615 [==============================] - 23s 37ms/step - loss: 0.2062 - accuracy: 0.9212 - val_loss: 0.7765 - val_accuracy: 0.7261\n",
      "Epoch 27/100\n",
      "615/615 [==============================] - 23s 37ms/step - loss: 0.2051 - accuracy: 0.9208 - val_loss: 0.7792 - val_accuracy: 0.7117\n",
      "Epoch 28/100\n",
      "615/615 [==============================] - 23s 37ms/step - loss: 0.1881 - accuracy: 0.9292 - val_loss: 0.8428 - val_accuracy: 0.7058\n",
      "Epoch 29/100\n",
      "615/615 [==============================] - 23s 37ms/step - loss: 0.1818 - accuracy: 0.9308 - val_loss: 0.9332 - val_accuracy: 0.7062\n",
      "Epoch 30/100\n",
      "615/615 [==============================] - 23s 37ms/step - loss: 0.1786 - accuracy: 0.9284 - val_loss: 0.8555 - val_accuracy: 0.7074\n",
      "Epoch 31/100\n",
      "615/615 [==============================] - 23s 37ms/step - loss: 0.1624 - accuracy: 0.9387 - val_loss: 0.8578 - val_accuracy: 0.7214\n",
      "Epoch 32/100\n",
      "615/615 [==============================] - 23s 37ms/step - loss: 0.1586 - accuracy: 0.9406 - val_loss: 0.8520 - val_accuracy: 0.7186\n",
      "Epoch 33/100\n",
      "615/615 [==============================] - 23s 37ms/step - loss: 0.1467 - accuracy: 0.9467 - val_loss: 0.9918 - val_accuracy: 0.7131\n",
      "Epoch 34/100\n",
      "615/615 [==============================] - 23s 37ms/step - loss: 0.1452 - accuracy: 0.9459 - val_loss: 0.9568 - val_accuracy: 0.7121\n",
      "Epoch 35/100\n",
      "615/615 [==============================] - 23s 37ms/step - loss: 0.1314 - accuracy: 0.9502 - val_loss: 1.0956 - val_accuracy: 0.7050\n",
      "Epoch 36/100\n",
      "615/615 [==============================] - 23s 37ms/step - loss: 0.1344 - accuracy: 0.9501 - val_loss: 0.9853 - val_accuracy: 0.7009\n",
      "Epoch 37/100\n",
      "615/615 [==============================] - 23s 37ms/step - loss: 0.1356 - accuracy: 0.9518 - val_loss: 0.9325 - val_accuracy: 0.7151\n",
      "Epoch 38/100\n",
      "615/615 [==============================] - 23s 37ms/step - loss: 0.1133 - accuracy: 0.9581 - val_loss: 1.0496 - val_accuracy: 0.7074\n",
      "Epoch 39/100\n",
      "615/615 [==============================] - 23s 37ms/step - loss: 0.1173 - accuracy: 0.9576 - val_loss: 0.9153 - val_accuracy: 0.7115\n",
      "Epoch 40/100\n",
      "615/615 [==============================] - 23s 37ms/step - loss: 0.1204 - accuracy: 0.9542 - val_loss: 1.1132 - val_accuracy: 0.7023\n",
      "Epoch 41/100\n",
      "615/615 [==============================] - 23s 37ms/step - loss: 0.1082 - accuracy: 0.9597 - val_loss: 1.0176 - val_accuracy: 0.7170\n",
      "Epoch 42/100\n",
      "615/615 [==============================] - 23s 37ms/step - loss: 0.1114 - accuracy: 0.9582 - val_loss: 1.0834 - val_accuracy: 0.6891\n",
      "Epoch 43/100\n",
      "615/615 [==============================] - 23s 37ms/step - loss: 0.1100 - accuracy: 0.9596 - val_loss: 1.2075 - val_accuracy: 0.6903\n",
      "Epoch 44/100\n",
      "615/615 [==============================] - 23s 37ms/step - loss: 0.1046 - accuracy: 0.9626 - val_loss: 1.0338 - val_accuracy: 0.7123\n",
      "Epoch 45/100\n",
      "615/615 [==============================] - 23s 37ms/step - loss: 0.1021 - accuracy: 0.9628 - val_loss: 1.1532 - val_accuracy: 0.6995\n",
      "Epoch 46/100\n",
      "615/615 [==============================] - 23s 37ms/step - loss: 0.1002 - accuracy: 0.9637 - val_loss: 1.0751 - val_accuracy: 0.7117\n",
      "Epoch 47/100\n",
      "615/615 [==============================] - 23s 37ms/step - loss: 0.0961 - accuracy: 0.9658 - val_loss: 1.1708 - val_accuracy: 0.6919\n",
      "Epoch 48/100\n",
      "615/615 [==============================] - 23s 37ms/step - loss: 0.0985 - accuracy: 0.9656 - val_loss: 1.1451 - val_accuracy: 0.7143\n",
      "Epoch 49/100\n",
      "615/615 [==============================] - 23s 37ms/step - loss: 0.0924 - accuracy: 0.9675 - val_loss: 1.2431 - val_accuracy: 0.6714\n",
      "Epoch 50/100\n",
      "615/615 [==============================] - 23s 37ms/step - loss: 0.0873 - accuracy: 0.9691 - val_loss: 1.2135 - val_accuracy: 0.7105\n",
      "Epoch 51/100\n",
      "615/615 [==============================] - 23s 37ms/step - loss: 0.0920 - accuracy: 0.9661 - val_loss: 1.1320 - val_accuracy: 0.7019\n",
      "Epoch 52/100\n",
      "615/615 [==============================] - 23s 37ms/step - loss: 0.0876 - accuracy: 0.9685 - val_loss: 1.2143 - val_accuracy: 0.6966\n",
      "Epoch 53/100\n",
      "615/615 [==============================] - 23s 37ms/step - loss: 0.0835 - accuracy: 0.9700 - val_loss: 1.1081 - val_accuracy: 0.6893\n",
      "Epoch 54/100\n",
      "615/615 [==============================] - 23s 37ms/step - loss: 0.0767 - accuracy: 0.9722 - val_loss: 1.1657 - val_accuracy: 0.7001\n",
      "Epoch 55/100\n",
      "615/615 [==============================] - 23s 37ms/step - loss: 0.0814 - accuracy: 0.9711 - val_loss: 1.0722 - val_accuracy: 0.7003\n",
      "Epoch 56/100\n",
      "615/615 [==============================] - 23s 37ms/step - loss: 0.0835 - accuracy: 0.9700 - val_loss: 1.1584 - val_accuracy: 0.6787\n",
      "Epoch 57/100\n",
      "615/615 [==============================] - 23s 37ms/step - loss: 0.0741 - accuracy: 0.9739 - val_loss: 1.1616 - val_accuracy: 0.7080\n",
      "Epoch 58/100\n",
      "615/615 [==============================] - 23s 37ms/step - loss: 0.0799 - accuracy: 0.9732 - val_loss: 1.4238 - val_accuracy: 0.7070\n",
      "Epoch 59/100\n",
      "615/615 [==============================] - 23s 37ms/step - loss: 0.0818 - accuracy: 0.9718 - val_loss: 1.2192 - val_accuracy: 0.6936\n",
      "Epoch 60/100\n",
      "615/615 [==============================] - 23s 37ms/step - loss: 0.0750 - accuracy: 0.9739 - val_loss: 1.2511 - val_accuracy: 0.7003\n",
      "Epoch 61/100\n",
      "615/615 [==============================] - 23s 37ms/step - loss: 0.0701 - accuracy: 0.9758 - val_loss: 1.1862 - val_accuracy: 0.7052\n",
      "Epoch 62/100\n",
      "615/615 [==============================] - 23s 37ms/step - loss: 0.0698 - accuracy: 0.9744 - val_loss: 1.2700 - val_accuracy: 0.6917\n",
      "Epoch 63/100\n",
      "615/615 [==============================] - 23s 37ms/step - loss: 0.0628 - accuracy: 0.9774 - val_loss: 1.3522 - val_accuracy: 0.7056\n",
      "Epoch 64/100\n",
      "615/615 [==============================] - 23s 37ms/step - loss: 0.0678 - accuracy: 0.9756 - val_loss: 1.3279 - val_accuracy: 0.6781\n",
      "Epoch 65/100\n",
      "615/615 [==============================] - 23s 37ms/step - loss: 0.0693 - accuracy: 0.9762 - val_loss: 1.1407 - val_accuracy: 0.7001\n",
      "Epoch 66/100\n",
      "615/615 [==============================] - 23s 37ms/step - loss: 0.0662 - accuracy: 0.9773 - val_loss: 1.3754 - val_accuracy: 0.6863\n",
      "Epoch 67/100\n",
      "615/615 [==============================] - 23s 37ms/step - loss: 0.0640 - accuracy: 0.9776 - val_loss: 1.3422 - val_accuracy: 0.7013\n",
      "Epoch 68/100\n",
      "615/615 [==============================] - 23s 37ms/step - loss: 0.0687 - accuracy: 0.9764 - val_loss: 1.3938 - val_accuracy: 0.6897\n",
      "Epoch 69/100\n",
      "615/615 [==============================] - 23s 37ms/step - loss: 0.0634 - accuracy: 0.9789 - val_loss: 1.3839 - val_accuracy: 0.6867\n",
      "Epoch 70/100\n",
      "615/615 [==============================] - 23s 37ms/step - loss: 0.0605 - accuracy: 0.9797 - val_loss: 1.3422 - val_accuracy: 0.7157\n",
      "Epoch 71/100\n",
      "615/615 [==============================] - 23s 37ms/step - loss: 0.0720 - accuracy: 0.9746 - val_loss: 1.5570 - val_accuracy: 0.6856\n",
      "Epoch 72/100\n",
      "615/615 [==============================] - 23s 37ms/step - loss: 0.0627 - accuracy: 0.9801 - val_loss: 1.5566 - val_accuracy: 0.6751\n",
      "Epoch 73/100\n",
      "615/615 [==============================] - 23s 37ms/step - loss: 0.0620 - accuracy: 0.9784 - val_loss: 1.3935 - val_accuracy: 0.6810\n",
      "Epoch 74/100\n",
      "615/615 [==============================] - 23s 37ms/step - loss: 0.0589 - accuracy: 0.9798 - val_loss: 1.4433 - val_accuracy: 0.6974\n",
      "Epoch 75/100\n",
      "615/615 [==============================] - 23s 37ms/step - loss: 0.0615 - accuracy: 0.9794 - val_loss: 1.4540 - val_accuracy: 0.6826\n",
      "Epoch 76/100\n",
      "615/615 [==============================] - 23s 37ms/step - loss: 0.0623 - accuracy: 0.9793 - val_loss: 1.2484 - val_accuracy: 0.6879\n",
      "Epoch 77/100\n",
      "615/615 [==============================] - 23s 37ms/step - loss: 0.0591 - accuracy: 0.9786 - val_loss: 1.4169 - val_accuracy: 0.6869\n",
      "Epoch 78/100\n",
      "615/615 [==============================] - 23s 37ms/step - loss: 0.0540 - accuracy: 0.9808 - val_loss: 1.3292 - val_accuracy: 0.6808\n",
      "Epoch 79/100\n",
      "615/615 [==============================] - 23s 37ms/step - loss: 0.0607 - accuracy: 0.9792 - val_loss: 1.4286 - val_accuracy: 0.6842\n",
      "Epoch 80/100\n",
      "615/615 [==============================] - 23s 37ms/step - loss: 0.0566 - accuracy: 0.9796 - val_loss: 1.2911 - val_accuracy: 0.6775\n",
      "Epoch 81/100\n",
      "615/615 [==============================] - 23s 37ms/step - loss: 0.0537 - accuracy: 0.9819 - val_loss: 1.3755 - val_accuracy: 0.6850\n",
      "Epoch 82/100\n",
      "615/615 [==============================] - 23s 37ms/step - loss: 0.0587 - accuracy: 0.9789 - val_loss: 1.3299 - val_accuracy: 0.6911\n",
      "Epoch 83/100\n",
      "615/615 [==============================] - 23s 37ms/step - loss: 0.0520 - accuracy: 0.9817 - val_loss: 1.5868 - val_accuracy: 0.6818\n",
      "Epoch 84/100\n",
      "615/615 [==============================] - 23s 37ms/step - loss: 0.0529 - accuracy: 0.9825 - val_loss: 1.3883 - val_accuracy: 0.6889\n",
      "Epoch 85/100\n",
      "615/615 [==============================] - 23s 37ms/step - loss: 0.0536 - accuracy: 0.9816 - val_loss: 1.3500 - val_accuracy: 0.6926\n",
      "Epoch 86/100\n",
      "615/615 [==============================] - 23s 37ms/step - loss: 0.0554 - accuracy: 0.9816 - val_loss: 1.4208 - val_accuracy: 0.6759\n",
      "Epoch 87/100\n",
      "615/615 [==============================] - 23s 37ms/step - loss: 0.0521 - accuracy: 0.9828 - val_loss: 1.4341 - val_accuracy: 0.6704\n",
      "Epoch 88/100\n",
      "615/615 [==============================] - 23s 37ms/step - loss: 0.0526 - accuracy: 0.9832 - val_loss: 1.4827 - val_accuracy: 0.6751\n",
      "Epoch 89/100\n",
      "615/615 [==============================] - 23s 37ms/step - loss: 0.0557 - accuracy: 0.9812 - val_loss: 1.4609 - val_accuracy: 0.6891\n",
      "Epoch 90/100\n",
      "615/615 [==============================] - 23s 37ms/step - loss: 0.0471 - accuracy: 0.9836 - val_loss: 1.5432 - val_accuracy: 0.6757\n",
      "Epoch 91/100\n",
      "615/615 [==============================] - 23s 37ms/step - loss: 0.0481 - accuracy: 0.9836 - val_loss: 1.4744 - val_accuracy: 0.6822\n",
      "Epoch 92/100\n",
      "615/615 [==============================] - 23s 37ms/step - loss: 0.0435 - accuracy: 0.9841 - val_loss: 1.4590 - val_accuracy: 0.6899\n",
      "Epoch 93/100\n",
      "615/615 [==============================] - 23s 37ms/step - loss: 0.0559 - accuracy: 0.9811 - val_loss: 1.4659 - val_accuracy: 0.6781\n",
      "Epoch 94/100\n",
      "615/615 [==============================] - 23s 37ms/step - loss: 0.0496 - accuracy: 0.9829 - val_loss: 1.5895 - val_accuracy: 0.6795\n",
      "Epoch 95/100\n",
      "615/615 [==============================] - 23s 37ms/step - loss: 0.0489 - accuracy: 0.9839 - val_loss: 1.4237 - val_accuracy: 0.7005\n",
      "Epoch 96/100\n",
      "615/615 [==============================] - 23s 37ms/step - loss: 0.0459 - accuracy: 0.9850 - val_loss: 1.6828 - val_accuracy: 0.6722\n",
      "Epoch 97/100\n",
      "615/615 [==============================] - 23s 37ms/step - loss: 0.0533 - accuracy: 0.9832 - val_loss: 1.5485 - val_accuracy: 0.6793\n",
      "Epoch 98/100\n",
      "615/615 [==============================] - 23s 37ms/step - loss: 0.0454 - accuracy: 0.9843 - val_loss: 1.4221 - val_accuracy: 0.6789\n",
      "Epoch 99/100\n",
      "615/615 [==============================] - 23s 37ms/step - loss: 0.0518 - accuracy: 0.9826 - val_loss: 1.6188 - val_accuracy: 0.6901\n",
      "Epoch 100/100\n",
      "615/615 [==============================] - 23s 37ms/step - loss: 0.0529 - accuracy: 0.9836 - val_loss: 1.5441 - val_accuracy: 0.6887\n",
      "Execution time: 0 hours, 38 minutes, 12 seconds\n"
     ]
    }
   ],
   "source": [
    "history = model_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839b92bd-5bee-4184-9fe2-abb8b51b8bc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "beeaea7c-dcc4-4663-8c34-8fcefea5d585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the history object to a pickle file\n",
    "with open('history.pkl', 'wb') as f:\n",
    "    pickle.dump(history.history, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "157f4284-5159-4213-814b-10d9108244e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_training_info():\n",
    "    # Set the model name\n",
    "    model_name = model.name\n",
    "    \n",
    "    # Get the current date and time\n",
    "    now = datetime.datetime.now()\n",
    "    date_time = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    # Save the history object to a CSV file\n",
    "    with open(str(name)+'-training_history.csv', 'a', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "\n",
    "        # Write header row if file is empty\n",
    "        if f.tell() == 0:\n",
    "            writer.writerow(['Model', 'Epoch', 'Train Loss', 'Train Acc', 'Val Loss', 'Val Acc', 'Date/Time', 'Video Name','Comment'])\n",
    "\n",
    "        # Write data for each epoch\n",
    "        for i, (tl, ta, vl, va) in enumerate(zip(history.history['loss'], history.history['accuracy'], history.history['val_loss'], history.history['val_accuracy'])):\n",
    "            writer.writerow([model_name, i+1, tl, ta, vl, va, date_time, video_name, 'without frame subtraction'])\n",
    "        writer.writerow(['', '', '', '', '', '', '', '' , ''])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "abb53f97-02c8-459a-b1ec-1041f808db98",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_training_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8a2d7428-a621-4d93-abf7-cecd2a6dcb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save train_images and train_labels\n",
    "with open('train_images.pkl', 'wb') as f:\n",
    "    pickle.dump(train_images, f)\n",
    "# save train_images and train_labels\n",
    "with open('val_images.pkl', 'wb') as f:\n",
    "    pickle.dump(val_images, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "388c4fdb-7c69-4dd6-95f9-2a3fdd63bd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_labels.pkl', 'wb') as f:\n",
    "    pickle.dump(train_labels, f)\n",
    "with open('val_labels.pkl', 'wb') as f:\n",
    "    pickle.dump(val_labels, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6d5fd999-a2ae-4cd0-8701-bdcfbc35bcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the accuracy of the model during training and XvalidationX\n",
    "# def plot_accuracy():\n",
    "#     plt.plot(history.history['accuracy'])\n",
    "#     plt.title('Model accuracy')\n",
    "#     plt.ylabel('Accuracy')\n",
    "#     plt.xlabel('Epoch')\n",
    "#     plt.legend(['Train'], loc='upper left')\n",
    "#     plt.savefig('accuracy.png')\n",
    "#     return plt.show()\n",
    "\n",
    "# def plot_loss():\n",
    "#     plt.plot(history.history['loss'])\n",
    "#     plt.title('Model loss')\n",
    "#     plt.ylabel('loss')\n",
    "#     plt.xlabel('Epoch')\n",
    "#     plt.legend(['Train'], loc='upper left')\n",
    "#     plt.savefig('loss.png')\n",
    "#     return plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a39a2943-28f0-4c20-b960-78d383293d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_accuracy()\n",
    "# plot_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e43045-42d0-4f41-8add-ac3e5b08dd82",
   "metadata": {},
   "source": [
    "High bias: If the training accuracy is low, it suggests that the model is underfitting the training data, i.e., it is not complex enough to capture the patterns in the data. In this case, you may need to increase the model's complexity by adding more layers or neurons, or by using a more complex architecture.\n",
    "\n",
    "High variance: If the training accuracy is high but the validation accuracy is low, it suggests that the model is overfitting the training data, i.e., it is memorizing the training data instead of generalizing to new data. In this case, you may need to use regularization techniques like dropout or L2 regularization, or use early stopping to prevent the model from overfitting.\n",
    "\n",
    "Good fit: If the training accuracy and validation accuracy are both high and close to each other, it suggests that the model is neither underfitting nor overfitting the data, i.e., it is generalizing well to new data.\n",
    "\n",
    "Plateauing: If the validation accuracy is no longer increasing as the training set size or epochs increase, it suggests that the model has reached its capacity and adding more data or epochs is unlikely to improve its performance.\n",
    "\n",
    "In general, a model accuracy curve can help you diagnose issues with your model and guide you in selecting appropriate strategies to improve its performance. It can also give you an idea of how much training data or how many epochs you need to achieve good performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1c658eb9-2008-4398-82f9-e34a230b7cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_confusion_matrix(model, images, labels, classes, title):\n",
    "#     # Predict the class labels using the model\n",
    "#     predicted_labels = np.argmax(model.predict(images), axis=1)\n",
    "\n",
    "#     # Compute the confusion matrix using the predicted class labels and the true class labels\n",
    "#     confusion = confusion_matrix(labels, predicted_labels)\n",
    "\n",
    "#     # Plot the confusion matrix\n",
    "#     fig, ax = plt.subplots(figsize=(10,10))\n",
    "#     ax.imshow(confusion)\n",
    "#     ax.set_xticks(np.arange(len(classes)))\n",
    "#     ax.set_yticks(np.arange(len(classes)))\n",
    "#     ax.set_xticklabels(classes)\n",
    "#     ax.set_yticklabels(classes)\n",
    "#     ax.set_xlabel('Predicted')\n",
    "#     ax.set_ylabel('True')\n",
    "#     ax.set_title(title)\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1778f6fe-63fa-443f-922c-6d5399d3a000",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Plot the confusion matrix for the training set\n",
    "# plot_confusion_matrix(model, train_images, train_labels, num_classes, 'Confusion Matrix for Training Set')\n",
    "\n",
    "# # Plot the confusion matrix for the validation set\n",
    "# plot_confusion_matrix(model, val_images, val_labels, num_classes, 'Confusion Matrix for Validation Set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c6d846-2b61-4f68-a2ae-279a57b48e7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312f730b-f468-4a63-b03a-b3a523cb3dd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431ae521-a723-4ec5-8971-719c5445f462",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d33d2a18-cee9-4f83-8e80-e89e1ef699c8",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Reflect on the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c7f0d1-8e62-4c31-8d65-d7e144301753",
   "metadata": {},
   "source": [
    "1. Insufficient data? One calcium video of 24186 frames and with 349x374 dimensions.\n",
    "2. Model architecture not appropriate. Try increasing the number of layers or filters, or adding more complex layers like BatchNormalization, Dropout, or Conv2DTranspose.\n",
    "3. Incorrect data preprocessing\n",
    "4. Incorrect hyperparameters\n",
    "5. Class Imbalance (do oversampling, or undersampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ae0ef6-1f9c-4afe-8c56-dd0563e9f96e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### [Ignore for now]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c9706562-e303-41cc-9e85-ec00240b827f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.layers import BatchNormalization\n",
    "# from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# model = Sequential()\n",
    "\n",
    "# model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', input_shape=(img_height, img_width, channel_dimension)))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# model.add(Conv2D(256, kernel_size=(3, 3), activation='relu'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# model.add(Flatten())\n",
    "\n",
    "# model.add(Dense(256, activation='relu'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Dropout(0.5))\n",
    "\n",
    "# model.add(Dense(128, activation='relu'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Dropout(0.5))\n",
    "\n",
    "# model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# # reshape train_images to have 4 dimensions\n",
    "# train_images = np.expand_dims(train_images, axis=-1)\n",
    "\n",
    "# # Reshape train_images to have 4 dimensions\n",
    "# #train_images = np.squeeze(train_images)\n",
    "# # train_images = np.squeeze(train_images, axis=-1)\n",
    "# # train_images = np.squeeze(train_images, axis=-1)\n",
    "# # train_images = np.squeeze(train_images, axis=-1)\n",
    "# # train_images = np.expand_dims(train_images, axis=-1)\n",
    "\n",
    "\n",
    "# # Data augmentation\n",
    "# train_datagen = ImageDataGenerator(rotation_range=10, width_shift_range=0.1, height_shift_range=0.1, \n",
    "#                                    shear_range=0.1, zoom_range=0.1, horizontal_flip=True, fill_mode='nearest')\n",
    "\n",
    "# history = model.fit(train_datagen.flow(train_images, train_labels, batch_size=batch_size),\n",
    "#                     epochs=epochs,\n",
    "#                     steps_per_epoch=len(train_images) // batch_size,\n",
    "#                     shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b192bf-a682-48c0-bcf4-f30847b0da8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "94ae0843-9623-4e7f-8e88-a647d9406d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reusable snippets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e9b0684b-764a-418c-a033-ff5fe9ce130b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load calcium video from local environment\n",
    "# with h5py.File('path', 'r') as f:\n",
    "#     video_data = np.array(f['analysis/recording_20211016_163921-PP-BP-MC/data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "72b34b51-5633-42e3-8399-6842489b6bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading locally\n",
    "# with h5py.File('/Users/konstantinoskalaitzidis/Developer/dmc/thesis_data/20211016_163921_animal1learnday1.h5', 'r') as f:\n",
    "#     print(list(f.keys()))\n",
    "#     behavior_data = np.array(f['per_frame'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "22276208-f41e-4589-9672-50de6e38d4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save the model architecture to a JSON file\n",
    "# with open('model_architecture.json', 'w') as f:\n",
    "#     f.write(model.to_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "05dc765c-32f7-4a47-bc62-518a371b26b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load the model architecture from the JSON file\n",
    "# with open('model_architecture.json', 'r') as f:\n",
    "#     json_string = f.read()\n",
    "\n",
    "# model_json = model_from_json(json_string)\n",
    "\n",
    "# # print the loaded model summary\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2f5af676-d833-4e4a-a4c3-3055adb4ce90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mySession = readSessionServer.SessionIterator()\n",
    "# sess = mySession.findSession()\n",
    "# # for sess in mySession.findSessions():\n",
    "# #     print(sess)\n",
    "# if sess.hasBehavior() and sess.hasCalcium():\n",
    "#     behavior = sess.getBehaviorSegmentation(align_with_calcium=True).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d8fd9685-0c81-4a69-b3ee-ed6c91b9f7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # define paths\n",
    "# video_path = '/Users/konstantinoskalaitzidis/Developer/dmc/thesis_data/20211016_163921_animal1learnday1.nwb'\n",
    "# train_dir = '/Users/konstantinoskalaitzidis/Developer/dmc/thesis_data/train'\n",
    "# test_dir = '/Users/konstantinoskalaitzidis/Developer/dmc/thesis_data/test'\n",
    "\n",
    "# # define train-test split ratio\n",
    "# train_test_ratio = 0.8\n",
    "\n",
    "# # open video file\n",
    "# cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# # get video frame count\n",
    "# frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "# # create list of frame indices\n",
    "# frame_indices = list(range(frame_count))\n",
    "\n",
    "# # shuffle frame indices\n",
    "# random.shuffle(frame_indices)\n",
    "\n",
    "# # split frame indices into train and test sets\n",
    "# train_frame_indices = frame_indices[:int(frame_count * train_test_ratio)]\n",
    "# test_frame_indices = frame_indices[int(frame_count * train_test_ratio):]\n",
    "\n",
    "# # iterate over frames and save to train or test directory\n",
    "# for i in range(frame_count):\n",
    "#     # read frame\n",
    "#     ret, frame = cap.read()\n",
    "#     if not ret:\n",
    "#         break\n",
    "    \n",
    "#     # save frame to train or test directory\n",
    "#     if i in train_frame_indices:\n",
    "#         cv2.imwrite(os.path.join(train_dir, f'{i}.jpg'), frame)\n",
    "#     else:\n",
    "#         cv2.imwrite(os.path.join(test_dir, f'{i}.jpg'), frame)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_kostas_env",
   "language": "python",
   "name": "new_kostas_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
