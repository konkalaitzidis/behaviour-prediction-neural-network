{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01485e77-4dcd-4196-930a-d748d4dfa097",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e82ea39-5b6c-4c12-a725-4e8c43abb055",
   "metadata": {
    "tags": []
   },
   "source": [
    "NOTE: All dependencies are within a conda environment to ensure reproducibility. To install all dependencies: pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eba8283c-185b-4595-82ad-2e1ac5ec5d97",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-19 19:15:54.262098: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-19 19:15:54.299433: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-19 19:15:55.116010: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-19 19:15:55.129310: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-19 19:15:55.129427: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "#Lets see if tensorflow finds the GPU\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "# import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4384096d-cb3e-4056-b096-8fa2252c12b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-19 19:15:55.132866: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-19 19:15:55.133450: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-19 19:15:55.133553: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-19 19:15:55.133629: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-19 19:15:55.439028: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-19 19:15:55.439143: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-19 19:15:55.439214: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-19 19:15:55.439272: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21697 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=float32, numpy=array([2.], dtype=float32)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# #Lets see if it works\n",
    "tf.ones(1) + tf.ones(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc9f33ac-2298-44bc-addc-fc0b862c345c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np # for working with arrays and matrices\n",
    "import pandas as pd # for data manipulation and analysis\n",
    "import matplotlib.pyplot as plt # for data visualization\n",
    "import seaborn as sns # for data visualization\n",
    "import time # for time-related functions\n",
    "import random # for random number generation\n",
    "import cv2 # for computer vision and image processing tasks\n",
    "import datetime # for saving date and time information\n",
    "import csv # for loading csv files\n",
    "\n",
    "\n",
    "import h5py # for working with HDF5 (Hierarchical Data Format) files\n",
    "import boto3 # for working with Amazon Web Services (AWS)\n",
    "from pynwb import NWBHDF5IO # for working with Neurodata Without Border (NWB) files\n",
    "import fsspec \n",
    "from fsspec.implementations.cached import CachingFileSystem # library used for working with various file systems in Python.\n",
    "import requests \n",
    "import aiohttp # libraries which are used for making HTTP requests in Python.\n",
    "import os # OS module provides various operating system-related functions to the code\n",
    "# import csv # CSV module is used for working with CSV (Comma Separated Values) files in Python.\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "\n",
    "# used for splitting data into training and testing sets in Python.\n",
    "from sklearn.model_selection import train_test_split \n",
    "# for generating a confusion matrix\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "\n",
    "# # Classes and functions from the Keras library which is used for building and training deep learning models in Python.\n",
    "# from keras.models import load_model\n",
    "# from keras.models import model_from_json\n",
    "# from keras.utils import to_categorical\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dropout\n",
    "# from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "# These import the Adam optimizer class and various other classes from the TensorFlow Keras library \n",
    "# which is a high-level neural networks API used for building and training deep learning models in Python.\n",
    "# from tensorflow.keras.optimizers.legacy import Adam\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from tensorflow.keras.utils import plot_model\n",
    "# from tensorflow.keras.callbacks import EarlyStopping\n",
    "import imgaug.augmenters as iaa\n",
    "\n",
    "\n",
    "\n",
    "# # Loading functions\n",
    "from load_calcium_video import load_video_data\n",
    "from pixel_values_normalization import normalize_video\n",
    "from align_behavior_to_calcium import align_files\n",
    "from class_balance import check_class_imbalance\n",
    "from model_architecture import construct_model\n",
    "from preprocessing_model import model_preprocessing\n",
    "from run_model import model_execution\n",
    "from save_model_info import save_training_info\n",
    "# from set_s3_connection import generate_s3_url\n",
    "from plots import plot_first_frames, plot_random_frames\n",
    "from send_email_when_code_is_run import send_email\n",
    "from class_balance import check_distribution_among_datasets\n",
    "\n",
    "\n",
    "from urllib.parse import urlparse\n",
    "from ann_visualizer.visualize import ann_viz\n",
    "import importlib\n",
    "import sys\n",
    "\n",
    "import IPython\n",
    "from IPython.display import display, Javascript\n",
    "\n",
    "\n",
    "\n",
    "from tensorflow.keras.applications.inception_v3 import preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca3632e-d9a1-49d1-89a8-ce3bee65331d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e671dd6b-a641-4394-b610-15af5966bce1",
   "metadata": {},
   "source": [
    "### Installing resnet-18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b86f181-82fc-4944-8d28-4b80716ef07e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6623baba-eaca-4135-a5ca-6071d0b37996",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce81aa90-7cdd-409d-b147-f5bdf50cc709",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6197c9d-f10e-40cc-8b12-da9d6cef45c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'experiment_ID' (str)\n"
     ]
    }
   ],
   "source": [
    "# Goal: Document pipeline with turning labels, for 20 epochs, and save the results appropriately. Use seaborn\n",
    "# Convert all plotting with seaborn\n",
    "experiment_name = \"Using another video, k-fold-cross validation, corridor location labels, 20% val - 80% train\"\n",
    "experiment_ID = '3.0'\n",
    "%store experiment_ID\n",
    "comment = \"Animal3learnday9\"+str(experiment_name)\n",
    "train_test_split_strategy = \"20% val - 80% train\"\n",
    "name = 'BPNN_V3'\n",
    "model_version = str(name)+'_1'\n",
    "# experiment_version = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "128440b6-9313-4558-9fd8-33fbd4a247ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The directory output already exists.\n"
     ]
    }
   ],
   "source": [
    "# Define the name of the output directory\n",
    "output_dir = \"output\"\n",
    "\n",
    "# Check if the output directory already exists\n",
    "if not os.path.exists(output_dir):\n",
    "    # Create the output directory\n",
    "    os.mkdir(output_dir)\n",
    "\n",
    "    # Create the balance, accuracy, loss, and cm directories inside the output directory\n",
    "    os.mkdir(os.path.join(output_dir, \"balance\"))\n",
    "    os.mkdir(os.path.join(output_dir, \"accuracy\"))\n",
    "    os.mkdir(os.path.join(output_dir, \"loss\"))\n",
    "    os.mkdir(os.path.join(output_dir, \"cm\"))\n",
    "    os.mkdir(os.path.join(output_dir, \"architecture\"))\n",
    "    os.mkdir(os.path.join(output_dir, \"pickles\"))\n",
    "else:\n",
    "    print(f\"The directory {output_dir} already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6d055e-0c52-4bcf-80f6-00594de77c4c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## [Start here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f789291d-67cc-4e37-b525-b973b1759103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load the CSV file with the FOV information\n",
    "# fov_info = pd.read_csv('/home/dmc/Desktop/kostas/direct-Behavior-prediction-from-miniscope-calcium-imaging-using-convolutional-neural-networks/src/V3/aligned_videos_animal3.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "361cee39-83bf-4db2-b375-c2af630d945f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 0 hours, 0 minutes, 3 seconds\n"
     ]
    }
   ],
   "source": [
    "s3_calcium_url = 'https://s3.ki.se/dmc-striatum-arrowmaze/processed-data/miniscope-recordings/export-to-nwb/animal3learnday9/20211026_142935_animal3learnday9.nwb?AWSAccessKeyId=5AMYRX4EUZ0MV0276K24&Signature=YQ01gqlKA1P%2BMjgJngwZuh2FeQQ%3D&Expires=1681920699'\n",
    "video_data = load_video_data(s3_calcium_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b587563-f2b3-43ee-af6c-d0b929d43468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# video_data = tf.image.grayscale_to_rgb(video_data)\n",
    "# val_images = tf.image.grayscale_to_rgb(val_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49f9271c-58ef-4e8d-9350-ef7ce6b6c7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_frame = np.min(video_data, axis=0)\n",
    "video_data = video_data - min_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b188b77-8a76-4f36-8255-4d4a27a2415a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum pixel value: 0.000\n",
      "Maximum pixel value: 1.0\n"
     ]
    }
   ],
   "source": [
    "images = normalize_video(video_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d60496b-93cd-41b7-a86f-d7e7a72641bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731fc004-e280-4012-82c2-ca0392889fbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c59fa02-7326-4d2e-bc20-ac8a6b1ad650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# session_video_data = {}\n",
    "# s3_calcium_url_list = ['https://s3.ki.se/dmc-striatum-arrowmaze/processed-data/miniscope-recordings/export-to-nwb/animal3learnday11/20211028_181307_animal3learnday11.nwb?AWSAccessKeyId=5AMYRX4EUZ0MV0276K24&Signature=LEjcw1bFE3zkfFj8YL8UWgcCtJ8%3D&Expires=1681745157']\n",
    "#                        # 'https://s3.ki.se/dmc-striatum-arrowmaze/processed-data/miniscope-recordings/export-to-nwb/animal3learnday10/20211027_165052_animal3learnday10.nwb?AWSAccessKeyId=5AMYRX4EUZ0MV0276K24&Signature=wvSHz9bYkQZ3k1RX%2BN4O9urXXRo%3D&Expires=1681742213']\n",
    "\n",
    "# for s3_calcium_url in s3_calcium_url_list:\n",
    "#     session_name = os.path.basename(os.path.dirname(urlparse(s3_calcium_url).path))\n",
    "#     print(session_name)\n",
    "#     video_data = load_video_data(s3_calcium_url)\n",
    "#     # session_video_data[session_name] = video_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc29269-acb7-43af-b4ae-5466e5da565c",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Load all videos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f487043f-f617-47fd-9488-46f771254158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# s3_calcium_url = ''\n",
    "# name_animal3learnday9 = os.path.basename(os.path.dirname(urlparse(s3_calcium_url).path))\n",
    "# print(name_animal3learnday9) \n",
    "# animal3learnday9 = load_video_data(s3_calcium_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e378c962-5c93-408d-8d3b-dbf38dff6b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# s3_calcium_url = 'hattps://s3.ki.se/dmc-striatum-arrowmaze/processed-data/miniscope-recordings/export-to-nwb/animal3learnday11/20211028_181307_animal3learnday11.nwb?AWSAccessKeyId=5AMYRX4EUZ0MV0276K24&Signature=VAAKMB1%2FKndaAHOtraSaY3fKnV0%3D&Expires=1681472165'\n",
    "# name_animal3learnday11 = os.path.basename(os.path.dirname(urlparse(s3_calcium_url).path))\n",
    "# print(name_animal3learnday11) \n",
    "# name_animal3learnday11 = load_video_data(s3_calcium_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e05ce69-9235-412d-a98c-41ea02c4c9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# s3_calcium_url = ''\n",
    "# name_animal3learnday10 = os.path.basename(os.path.dirname(urlparse(s3_calcium_url).path))\n",
    "# print(name_animal3learnday10) \n",
    "# animal3learnday10 = load_video_data(s3_calcium_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8e4213a-b795-4cf7-ac9a-714e4f1da0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# s3_calcium_urls_dict = {\n",
    "#     'animal3learnday9': 'https://s3.ki.se/dmc-striatum-arrowmaze/processed-data/miniscope-recordings/export-to-nwb/animal3learnday9/20211026_142935_animal3learnday9.nwb?AWSAccessKeyId=5AMYRX4EUZ0MV0276K24&Signature=GYmPHe6%2BzTK41rlQK6Pi9WwKtqk%3D&Expires=1681476565',\n",
    "#     'animal3learnday10': 'https://s3.ki.se/dmc-striatum-arrowmaze/processed-data/miniscope-recordings/export-to-nwb/animal3learnday10/20211027_165052_animal3learnday10.nwb?AWSAccessKeyId=5AMYRX4EUZ0MV0276K24&Signature=4FaazGe6UnNxn10E9xH%2Bbivo61U%3D&Expires=1681476574',\n",
    "#     'animal3learnday11': 'https://s3.ki.se/dmc-striatum-arrowmaze/processed-data/miniscope-recordings/export-to-nwb/animal3learnday11/20211028_181307_animal3learnday11.nwb?AWSAccessKeyId=5AMYRX4EUZ0MV0276K24&Signature=VAAKMB1%2FKndaAHOtraSaY3fKnV0%3D&Expires=1681472165'\n",
    "# }\n",
    "\n",
    "# # smth_name = [str(20211026_142935), str(20211027_165052),  str(20211028_181307)]\n",
    "\n",
    "# # list of multiple urls\n",
    "# # s3_calcium_urls = ['', '', 'https://s3.ki.se/dmc-striatum-arrowmaze/processed-data/miniscope-recordings/export-to-nwb/animal3learnday11/20211028_181307_animal3learnday11.nwb?AWSAccessKeyId=5AMYRX4EUZ0MV0276K24&Signature=VAAKMB1%2FKndaAHOtraSaY3fKnV0%3D&Expires=1681472165']\n",
    "# video_data_list = []  # create empty list to store video data\n",
    "\n",
    "# for name, url in s3_calcium_urls_dict.items():\n",
    "#     # name_animal = os.path.basename(os.path.dirname(urlparse(s3_calcium_urls).path))\n",
    "#     print(name) \n",
    "#     video_data = load_video_data(url)\n",
    "#     video_data_list.append(video_data)  # append video data to the list\n",
    "\n",
    "# printt(video_data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "74ab2329-17f4-4db6-a215-ee76fb0a3be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv_file = '/home/dmc/Desktop/kostas/direct-Behavior-prediction-from-miniscope-calcium-imaging-using-convolutional-neural-networks/src/V2/aligned_videos_animal3.csv'  # path to your CSV file\n",
    "\n",
    "# video_info = []  # create empty list to store video information\n",
    "\n",
    "# with open(csv_file, newline='') as f:\n",
    "#     reader = csv.reader(f)\n",
    "#     for row in reader:\n",
    "#         video_name = row[8]  # extract video name\n",
    "#         crop_coords = tuple(map(int, row[1:]))  # extract crop coordinates and convert to tuple of integers\n",
    "#         video_info.append((video_name, crop_coords))  # append video name and crop coordinates as a tuple to the list\n",
    "\n",
    "# # Now video_info is a list of tuples, where each tuple contains the video name and its corresponding crop coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "694827e3-8e16-48a6-badd-b457d0a99589",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(394, 415)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "773fa1f5-a228-44e1-bfbf-446cb5013030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.expand_dims(images, -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "df3d9dc2-38ac-495a-b8f8-fa2ee5bb584b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.02954545, 0.03181818, 0.04090909, ..., 0.02727273,\n",
       "         0.01818182, 0.03181818],\n",
       "        [0.04772727, 0.05      , 0.05454545, ..., 0.03181818,\n",
       "         0.02954545, 0.03863636],\n",
       "        [0.06590909, 0.03636364, 0.04772727, ..., 0.05      ,\n",
       "         0.04318182, 0.04318182],\n",
       "        ...,\n",
       "        [0.10227273, 0.09090909, 0.08409091, ..., 0.09318182,\n",
       "         0.05      , 0.06136364],\n",
       "        [0.04545455, 0.10227273, 0.07954545, ..., 0.075     ,\n",
       "         0.04545455, 0.05      ],\n",
       "        [0.06818182, 0.07727273, 0.08181818, ..., 0.06363636,\n",
       "         0.04318182, 0.04090909]]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if the video has been loaded correctly and normalized\n",
    "images[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab23843d-4105-4cab-ad98-093127d38875",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cb5e2e43-31a0-454a-8264-5227aef46851",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of video frames is  24434  and the frame dimensions (height x width) are:  394 X 415\n"
     ]
    }
   ],
   "source": [
    "# Determine the size of the calcium video dataset\n",
    "num_of_frames = images.shape[0]\n",
    "img_height = images.shape[1]\n",
    "img_width = images.shape[2]\n",
    "print(\"The number of video frames is \", num_of_frames, \" and the frame dimensions (height x width) are: \", img_height, \"X\", img_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0e0e8e73-b364-41ba-bc6c-77297c1c7bda",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Preparing bonsai data file.\n",
    "# CSV with additional data from the behavior box, such as reward deliveries. Also includes information needed for synchronizing the calcium and behavioral recordings.\n",
    "# bonsai_data = pd.read_csv('/home/dmc/Desktop/kostas/direct-Behavior-prediction-from-miniscope-calcium-imaging-using-convolutional-neural-networks/data/tmaze_2021-10-28T18_13_23.csv', header=None)\n",
    "bonsai_data = pd.read_csv('/home/dmc/Desktop/kostas/direct-Behavior-prediction-from-miniscope-calcium-imaging-using-convolutional-neural-networks/data/tmaze_2021-10-26T14_29_27.csv', header=None)\n",
    "\n",
    "# Segmentation of each frame into one behavior class.\n",
    "# df_behavior = pd.read_hdf('/home/dmc/Desktop/kostas/direct-Behavior-prediction-from-miniscope-calcium-imaging-using-convolutional-neural-networks/data/20211028_181307_animal3learnday11.h5', 'per_frame')\n",
    "df_behavior = pd.read_hdf('/home/dmc/Desktop/kostas/direct-Behavior-prediction-from-miniscope-calcium-imaging-using-convolutional-neural-networks/data/20211026_142935_animal3learnday9.h5', 'per_frame')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "15994e40-6986-4eb3-b608-445c51b4d14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_new_annotations, df_unique_states = align_files(bonsai_data, df_behavior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5488fc16-ac4a-4321-b7b5-c2e53041b37d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-10-26T14:29:29.7238016+02:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>57</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-10-26T14:29:29.7238528+02:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>57</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-10-26T14:29:29.7238784+02:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>57</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-10-26T14:29:29.7238912+02:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>57</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-10-26T14:29:29.7238912+02:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>57</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   0  1  2   3      4      5      6  7\n",
       "0  2021-10-26T14:29:29.7238016+02:00  0  0  57  False  False  False  0\n",
       "1  2021-10-26T14:29:29.7238528+02:00  0  0  57  False  False  False  0\n",
       "2  2021-10-26T14:29:29.7238784+02:00  0  0  57  False  False  False  0\n",
       "3  2021-10-26T14:29:29.7238912+02:00  0  0  57  False  False  False  0\n",
       "4  2021-10-26T14:29:29.7238912+02:00  0  0  57  False  False  False  0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bonsai_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "91970ff1-58b7-41de-b951-cff32c5dfb33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>Trial_Number</th>\n",
       "      <th>Reward</th>\n",
       "      <th>Frame_Number</th>\n",
       "      <th>Central_Zone</th>\n",
       "      <th>L_Zone</th>\n",
       "      <th>R_Zone</th>\n",
       "      <th>Calcium_frame</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-10-26T14:29:29.7238016+02:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>57</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-10-26T14:29:29.7238528+02:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>57</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-10-26T14:29:29.7238784+02:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>57</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-10-26T14:29:29.7238912+02:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>57</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-10-26T14:29:29.7238912+02:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>57</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Time  Trial_Number  Reward  Frame_Number  \\\n",
       "0  2021-10-26T14:29:29.7238016+02:00             0       0            57   \n",
       "1  2021-10-26T14:29:29.7238528+02:00             0       0            57   \n",
       "2  2021-10-26T14:29:29.7238784+02:00             0       0            57   \n",
       "3  2021-10-26T14:29:29.7238912+02:00             0       0            57   \n",
       "4  2021-10-26T14:29:29.7238912+02:00             0       0            57   \n",
       "\n",
       "   Central_Zone  L_Zone  R_Zone  Calcium_frame  \n",
       "0         False   False   False              0  \n",
       "1         False   False   False              0  \n",
       "2         False   False   False              0  \n",
       "3         False   False   False              0  \n",
       "4         False   False   False              0  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adding column names\n",
    "bonsai_data = bonsai_data.rename(columns={\n",
    "    0: 'Time', 1: 'Trial_Number',\n",
    "    2: 'Reward', 3: 'Frame_Number', 4: 'Central_Zone',\n",
    "    5: 'L_Zone', 6: 'R_Zone', 7: 'Calcium_frame'})\n",
    "bonsai_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0183aa33-8575-41fb-ad60-6b801d7fb192",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>frame_no</th>\n",
       "      <th>segment_no</th>\n",
       "      <th>state_id</th>\n",
       "      <th>spatial_progress</th>\n",
       "      <th>temporal_progress</th>\n",
       "      <th>state_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>717</td>\n",
       "      <td>717</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>0.316</td>\n",
       "      <td>0.316</td>\n",
       "      <td>initReward</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>720</td>\n",
       "      <td>720</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>0.322</td>\n",
       "      <td>0.322</td>\n",
       "      <td>initReward</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>723</td>\n",
       "      <td>723</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>0.328</td>\n",
       "      <td>0.328</td>\n",
       "      <td>initReward</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>726</td>\n",
       "      <td>726</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>0.334</td>\n",
       "      <td>0.334</td>\n",
       "      <td>initReward</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>729</td>\n",
       "      <td>729</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>0.340</td>\n",
       "      <td>0.340</td>\n",
       "      <td>initReward</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  frame_no  segment_no  state_id  spatial_progress  temporal_progress  \\\n",
       "0    717       717          24         0             0.316              0.316   \n",
       "1    720       720          24         0             0.322              0.322   \n",
       "2    723       723          24         0             0.328              0.328   \n",
       "3    726       726          24         0             0.334              0.334   \n",
       "4    729       729          24         0             0.340              0.340   \n",
       "\n",
       "   state_name  \n",
       "0  initReward  \n",
       "1  initReward  \n",
       "2  initReward  \n",
       "3  initReward  \n",
       "4  initReward  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_aligned = df_behavior.loc[bonsai_data.groupby('Calcium_frame').first()[1:].Frame_Number].reset_index()\n",
    "df_aligned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "101851be-7fc8-45fc-8686-c8250a3bd6df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state_id</th>\n",
       "      <th>state_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>initReward</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>initReward</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>initReward</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>initReward</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>initReward</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   state_id  state_name\n",
       "0         0  initReward\n",
       "1         0  initReward\n",
       "2         0  initReward\n",
       "3         0  initReward\n",
       "4         0  initReward"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new_annotations = df_aligned[['state_id', 'state_name']]\n",
    "df_new_annotations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c8c4a72a-4ea5-4e8c-ad94-f42bf728bd5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "state_id\n",
       "0          initReward\n",
       "1            initLeft\n",
       "2           initRight\n",
       "3         mainRunLeft\n",
       "4        mainRunRight\n",
       "5          mainReturn\n",
       "6           mainOther\n",
       "7      turnMainToLeft\n",
       "8     turnMainToRight\n",
       "9      turnLeftToMain\n",
       "10    turnRightToMain\n",
       "11    turnLeftToRight\n",
       "12    turnRightToLeft\n",
       "13            leftRun\n",
       "14         leftReturn\n",
       "15         leftReward\n",
       "16           leftLeft\n",
       "17          leftRight\n",
       "18          leftOther\n",
       "19           rightRun\n",
       "20        rightReturn\n",
       "21        rightReward\n",
       "22          rightLeft\n",
       "23         rightRight\n",
       "24         rightOther\n",
       "Name: state_name, dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_unique_states = df_new_annotations[['state_id', 'state_name']].drop_duplicates(subset='state_id').set_index('state_id')['state_name'].sort_index()\n",
    "df_unique_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ed696b72-eb03-4420-a58d-9d7a3897a467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# state id mapping for main corridor, left corridor, right corridor\n",
    "state_id_map = {\n",
    "    1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0,\n",
    "    9: 1, 11: 1, 13: 1, 14: 1, 15: 1, 16: 1, 17: 1, 18: 1,\n",
    "    10: 2, 12: 2, 19: 2, 20: 2, 21: 2, 22: 2, 23: 2, 24: 2,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7ad82aee-2f60-48e9-a8fd-0aec93782c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_37445/4243124794.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_new_annotations.loc[:, 'state_id'] = df_new_annotations.loc[:, 'state_id'].replace(state_id_map)\n"
     ]
    }
   ],
   "source": [
    "df_new_annotations.loc[:, 'state_id'] = df_new_annotations.loc[:, 'state_id'].replace(state_id_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "42a5a562-84d9-40e7-b432-308ed008ab85",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_annotations = df_new_annotations.loc[:, 'state_id']    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ecabe0-97f8-4a67-a94f-04fb5cfe756c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d2008895-5abe-4e2a-9544-c4b7e9b1caf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new_annotations_unique = df_new_annotations.unique()\n",
    "df_new_annotations_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c6e8f21f-729a-423c-b4d3-2db9266e190e",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"/home/dmc/Desktop/kostas/direct-Behavior-prediction-from-miniscope-calcium-imaging-using-convolutional-neural-networks/src/V3/output/balance\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "01334238-8426-46ae-a6d1-9a21382b7b1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Behavior Main Corridor is 36.4%\n",
      "Behavior Left Corridor is 41.2%\n",
      "Behavior Right Corridor is 22.4%\n"
     ]
    }
   ],
   "source": [
    "class_counts, total_counts = check_class_imbalance(df_new_annotations, experiment_ID, save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bab359-ef5c-483e-9060-aec8dc9b5f8c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Data Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "92ddf3ba-4189-4ef5-b77e-60cd885f6be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_images, val_images, train_labels, val_labels, num_classes = model_preprocessing(train_images, val_images, train_labels, val_labels, df_new_annotations_unique)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bca7751e-084b-4508-86e5-31fc09446830",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Training Parameters\n",
    "channel_dimension = 1\n",
    "labels = df_new_annotations\n",
    "epochs = 20\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ea14e836-bc2c-4da3-a31e-d4bfba70eb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "71528798-89e3-4dd7-80f4-5c239a587e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (img_height, img_width, channel_dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9a51d630-0706-495f-a76c-d93b56a0048a",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_behaviors = df_new_annotations_unique\n",
    "\n",
    "# Define the number of classes\n",
    "num_classes = len(no_of_behaviors)\n",
    "\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "labels = to_categorical(labels, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73ffd83-3c1e-406d-965f-c6abc478c476",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e5a7fdd8-52f9-4e03-a4c7-45899f43b4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up k-fold cross-validation\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "num_folds = 5\n",
    "kf = KFold(n_splits=num_folds, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1815d67-d425-44e3-8942-eb43c2374db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/5\n",
      "Model: \"BPNN_V3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 392, 413, 32)      320       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 196, 206, 32)     0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 194, 204, 64)      18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 97, 102, 64)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 633216)            0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 633216)            0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               81051776  \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 3)                 387       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 81,070,979\n",
      "Trainable params: 81,070,979\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Compiling model...\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-19 19:16:25.833663: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8401\n",
      "2023-04-19 19:16:26.948090: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "611/611 [==============================] - 27s 39ms/step - loss: 1.0147 - accuracy: 0.4792 - val_loss: 0.8711 - val_accuracy: 0.6086\n",
      "Epoch 2/20\n",
      "611/611 [==============================] - 21s 35ms/step - loss: 0.7657 - accuracy: 0.6607 - val_loss: 0.6509 - val_accuracy: 0.7409\n",
      "Epoch 3/20\n",
      "611/611 [==============================] - 21s 35ms/step - loss: 0.6323 - accuracy: 0.7313 - val_loss: 0.5947 - val_accuracy: 0.7592\n",
      "Epoch 4/20\n",
      "611/611 [==============================] - 21s 35ms/step - loss: 0.5186 - accuracy: 0.7872 - val_loss: 0.4753 - val_accuracy: 0.8052\n",
      "Epoch 5/20\n",
      "611/611 [==============================] - 21s 35ms/step - loss: 0.4347 - accuracy: 0.8236 - val_loss: 0.5029 - val_accuracy: 0.7995\n",
      "Epoch 6/20\n",
      "611/611 [==============================] - 21s 35ms/step - loss: 0.3567 - accuracy: 0.8591 - val_loss: 0.4081 - val_accuracy: 0.8392\n",
      "Epoch 7/20\n",
      "611/611 [==============================] - 21s 35ms/step - loss: 0.3057 - accuracy: 0.8785 - val_loss: 0.3637 - val_accuracy: 0.8586\n",
      "Epoch 8/20\n",
      "611/611 [==============================] - 21s 35ms/step - loss: 0.2522 - accuracy: 0.9012 - val_loss: 0.4043 - val_accuracy: 0.8465\n",
      "Epoch 9/20\n",
      "611/611 [==============================] - 21s 35ms/step - loss: 0.2093 - accuracy: 0.9182 - val_loss: 0.3526 - val_accuracy: 0.8658\n",
      "Epoch 10/20\n",
      "611/611 [==============================] - 21s 35ms/step - loss: 0.1774 - accuracy: 0.9321 - val_loss: 0.3291 - val_accuracy: 0.8711\n",
      "Epoch 11/20\n",
      "611/611 [==============================] - 21s 35ms/step - loss: 0.1463 - accuracy: 0.9443 - val_loss: 0.3384 - val_accuracy: 0.8748\n",
      "Epoch 12/20\n",
      "611/611 [==============================] - 21s 35ms/step - loss: 0.1332 - accuracy: 0.9493 - val_loss: 0.3198 - val_accuracy: 0.8789\n",
      "Epoch 13/20\n",
      "611/611 [==============================] - 21s 35ms/step - loss: 0.1156 - accuracy: 0.9567 - val_loss: 0.3188 - val_accuracy: 0.8854\n",
      "Epoch 14/20\n",
      "611/611 [==============================] - 21s 35ms/step - loss: 0.0927 - accuracy: 0.9657 - val_loss: 0.3619 - val_accuracy: 0.8760\n",
      "Epoch 15/20\n",
      "611/611 [==============================] - 21s 35ms/step - loss: 0.0851 - accuracy: 0.9680 - val_loss: 0.3189 - val_accuracy: 0.8983\n",
      "Epoch 16/20\n",
      "611/611 [==============================] - 21s 35ms/step - loss: 0.0779 - accuracy: 0.9719 - val_loss: 0.3306 - val_accuracy: 0.8854\n",
      "Epoch 17/20\n",
      "611/611 [==============================] - 21s 35ms/step - loss: 0.0698 - accuracy: 0.9743 - val_loss: 0.4876 - val_accuracy: 0.8717\n",
      "Epoch 18/20\n",
      "611/611 [==============================] - 21s 35ms/step - loss: 0.0662 - accuracy: 0.9764 - val_loss: 0.3599 - val_accuracy: 0.8713\n",
      "Epoch 19/20\n",
      "611/611 [==============================] - 21s 35ms/step - loss: 0.0642 - accuracy: 0.9772 - val_loss: 0.3968 - val_accuracy: 0.8840\n",
      "Epoch 20/20\n",
      "611/611 [==============================] - 21s 35ms/step - loss: 0.0488 - accuracy: 0.9832 - val_loss: 0.3806 - val_accuracy: 0.8885\n",
      "Test accuracy: 0.8885\n",
      "\n",
      "Fold 2/5\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate the model for each fold\n",
    "fold = 0\n",
    "for train_idx, test_idx in kf.split(images):\n",
    "    fold += 1\n",
    "    print(f'Fold {fold}/{num_folds}')\n",
    "    \n",
    "    # Split the data into training and test sets\n",
    "    train_images, train_labels = images[train_idx], labels[train_idx]\n",
    "    test_images, test_labels = images[test_idx], labels[test_idx]\n",
    "    \n",
    "    # Create a new instance of the model\n",
    "    # model = create_model(input_shape=train_images[0].shape, num_classes=3)\n",
    "    model = construct_model(input_shape, num_classes, name)\n",
    "    \n",
    "    # Train the model on the training set\n",
    "    model.fit(train_images, train_labels, epochs=epochs, batch_size=batch_size, validation_data=(test_images, test_labels))\n",
    "    \n",
    "    # Evaluate the model on the test set\n",
    "    score = model.evaluate(test_images, test_labels, verbose=0)\n",
    "    print(f'Test accuracy: {score[1]:.4f}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19487d63-c4bd-4174-a4b4-34f14bf61194",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70441d4-39a9-4548-a570-22f49dfc5cbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee565f24-cd9c-4f8e-b80e-8409dad0cf1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5861aa-5f29-4370-8493-74765c440f1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39bd661-97b4-4b32-aef6-195fde8a7cf4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f37321-bfc4-4e2a-bad3-c21a6cb17367",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e34e8b2-c88f-447c-94d0-7a5e0ca7447f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4974fb1-2409-4310-bc4e-cf8950c49ea9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b91aa4-2c6a-4590-82c6-5b21adbf7c45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15101b9e-d830-489e-a4db-1ffde2f342a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d682a8b4-b434-4bb7-98f9-ff4bb66ee5ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8259cc26-3be4-4b36-9c53-7e5a5f2faf96",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871ed3d0-e03e-40c0-81c3-413aeb397382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Split the data into training and validation sets\n",
    "split_index = int(0.2 * len(images))  # Index to split data\n",
    "\n",
    "# images = np.concatenate([images, images, images], axis=-1)\n",
    "\n",
    "val_images, train_images = images[:split_index], images[split_index:]\n",
    "val_labels, train_labels = labels[:split_index], labels[split_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e71b18-a3a9-4f2f-88c0-9bb8fa848a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_distribution_among_datasets(val_labels, experiment_ID, save_dir, dataset_type = 'Validation_set')\n",
    "check_distribution_among_datasets(train_labels, experiment_ID, save_dir, dataset_type = 'Training_set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c0888d-2546-4001-be62-9fb3c38ff632",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images, val_images, train_labels, val_labels, num_classes = model_preprocessing(train_images, val_images, train_labels, val_labels, df_new_annotations_unique)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df80a40-8405-4922-8b5f-306c6f2e7238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and validation sets\n",
    "# train_images, val_images, train_labels, val_labels = train_test_split(images, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77649636-6233-4d31-9f22-05a83cc38f57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vmin = 0\n",
    "vmax = 0.20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51c845a-ca63-489f-a54f-e3422eb1bbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot the first 5 random images\n",
    "plot_first_frames(train_images, train_labels, vmin, vmax)\n",
    "plot_first_frames(val_images, val_labels, vmin, vmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5b5d43-e986-4b94-823f-5a08b089ed05",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_random_frames(train_images, train_labels, vmin, vmax)\n",
    "plot_random_frames(val_images, val_labels, vmin, vmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad6f6e8-4bb2-4261-b1e5-ebbdf11f39ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (img_height, img_width, channel_dimension)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb359b74-1591-4a8a-a763-44c7a93f3876",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = construct_model(input_shape, num_classes, name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c920c3b-3253-4fc4-b140-9c403eb676a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data=(val_images, val_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e6e795-a15f-4c2e-8364-b2697e8135a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'model': model,\n",
    "    'tf': tf,\n",
    "    'train_images': train_images,\n",
    "    'train_labels': train_labels,\n",
    "    'epochs': epochs,\n",
    "    'batch_size': batch_size,\n",
    "    'validation_data': validation_data,\n",
    "    'val_images': validation_data[0],\n",
    "    'val_labels': validation_data[1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ab74a8-c336-4cf4-868a-a2a320b71eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"/home/dmc/Desktop/kostas/direct-Behavior-prediction-from-miniscope-calcium-imaging-using-convolutional-neural-networks/src/V3/output/pickles\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327225b7-559f-4dc3-bd35-af02855a6210",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model_execution(params, save_dir, model_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0b3cda-f710-4689-ae0e-f514b4a8ab46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3420a8a1-ec64-4e33-92c8-901c90ea493b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec45085-248f-4d9f-a5f7-0f782bec596a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6cac05-a437-472a-b070-8771e656bce0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa506e5-89ed-4908-a584-855a503b12d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7525aa63-3c5c-4f61-ae3d-6164fcf49f83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e334e3f6-8286-4ccf-ae19-5099f206fb25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_images, val_images, train_labels, val_labels, num_classes = model_preprocessing(train_images, val_images, train_labels, val_labels, df_new_annotations_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0b93cb-f5b3-40a2-a8e7-5eeb4a970af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_distribution_among_datasets(val_labels, experiment_ID, save_dir, dataset_type = 'Validation_set')\n",
    "check_distribution_among_datasets(train_labels, experiment_ID, save_dir, dataset_type = 'Training_set')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff0bae9-d6e8-4bf9-8312-52d975610389",
   "metadata": {},
   "source": [
    "#### Test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4b17e5-fa99-407d-8792-100aa47e2284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and validation sets\n",
    "# train_images, val_images, train_labels, val_labels = train_test_split(images.reshape(num_of_frames, img_height, img_width, channel_dimension), labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25748ac7-5471-4d95-b550-b4c52b7d5a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_model(X_train, y_train, X_val, y_val, input_shape, num_classes, model_version):\n",
    "    # Define the model\n",
    "    model = construct_model(input_shape, num_classes, name=f'model_version_{model_version}')\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    # Define early stopping callback\n",
    "    early_stop = EarlyStopping(patience=3, monitor='val_loss')\n",
    "\n",
    "    # Train the model on this fold\n",
    "    model.fit(X_train, y_train, validation_data=(X_val, y_val), batch_size=32, epochs=10, callbacks=[early_stop])\n",
    "\n",
    "    # Evaluate the model on the validation set for this fold\n",
    "    score = model.evaluate(X_val, y_val, verbose=0)\n",
    "    print(f\"Validation accuracy: {score[1]:.4f}\")\n",
    "\n",
    "    # Save the model for this fold\n",
    "    save_model(model, f'model_version_{model_version}.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84d71f9-a4a6-4efc-9935-3084aae65548",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold_cross_validation(train_images, train_labels, num_classes, input_shape):\n",
    "    # Define the number of folds\n",
    "    num_folds = 5\n",
    "\n",
    "    # Define the K-fold cross validator\n",
    "    kf = KFold(n_splits=num_folds, shuffle=True)\n",
    "\n",
    "    # Loop over the folds\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(train_images)):\n",
    "\n",
    "        # Define the training and validation data for this fold\n",
    "        X_train, y_train = train_images[train_idx], train_labels[train_idx]\n",
    "        X_val, y_val = train_images[val_idx], train_labels[val_idx]\n",
    "\n",
    "        # Train and evaluate the model\n",
    "        train_and_evaluate_model(X_train, y_train, X_val, y_val, input_shape, num_classes, fold+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514cb0ae-1623-4fb6-87f1-a17cefadfd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold_cross_validation(train_images, train_labels, num_classes, input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d26c26-e862-494f-89a2-5fd3c2c7cac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and validation sets\n",
    "train_images, val_images, train_labels, val_labels = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447df7f2-b71c-4e66-b365-38611920e5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function parameters\n",
    "params = {\n",
    "    'input_shape': input_shape,\n",
    "    'num_classes': num_classes,\n",
    "    'epochs': epochs,\n",
    "    'batch_size': batch_size,\n",
    "    'train_images': train_images,\n",
    "    'train_labels': train_labels,\n",
    "    'val_images': val_images,\n",
    "    'val_labels': val_labels\n",
    "}\n",
    "\n",
    "# Call the function\n",
    "train_and_evaluate_model(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf8fa30-73fb-4ae1-ad1d-5a3a0968a429",
   "metadata": {},
   "source": [
    "#### Subtract the minimum frame from all other frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3c0653-d660-43fc-bf1f-741401b557ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # min_frame = np.min(images, axis=0)\n",
    "# # background_removed_frames = []\n",
    "# # for frame in images:\n",
    "# #     background_removed_frames.append(frame - min_frame)\n",
    "    \n",
    "# # images = np.array(background_removed_frames)\n",
    "# # min_frame = np.min(train_images, axis=0)\n",
    "# # train_images = train_images - min_frame\n",
    "\n",
    "# min_frame_train = np.min(train_images, axis=0)\n",
    "# train_images = train_images - min_frame_train\n",
    "\n",
    "# min_frame_val = np.min(val_images, axis=0)\n",
    "# val_images = val_images - min_frame_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f885f6-1116-4629-9238-37eab4a2e6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# str(labels[2400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc66baf-ffcd-4649-b30a-15734a1b9db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sys.path.append('/home/dmc/Desktop/kostas/direct-Behavior-prediction-from-miniscope-calcium-imaging-using-convolutional-neural-networks/src/V2/plots.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2715432-82cb-4a3a-bf1f-884c33d533bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importlib.reload(plots.plot_first_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9af8c74-ad11-4988-a10d-4e48934b5cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_random_frames(train_images, train_labels, vmin, vmax)\n",
    "# plot_random_frames(val_images, val_labels, vmin, vmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3893d102-7214-46b7-b6c7-9467113fb4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_array = train_images[9000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d313af08-823a-4d2b-a7e5-c2f7ba4877b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# img = cv2.cvtColor(img_array, cv2.COLOR_GRAY2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27345e92-bd05-433c-abc2-120208c373b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Overlay pixel values on top of the image\n",
    "# for y in range(img_height):\n",
    "#     for x in range(img_width):\n",
    "#         value = img_array[y, x]\n",
    "#         text = str(value)\n",
    "#         font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "#         font_scale = 0.3\n",
    "#         thickness = 1\n",
    "#         color = (255, 255, 255)\n",
    "#         position = (x, y)\n",
    "#         cv2.putText(img, text, position, font, font_scale, color, thickness)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc6bf9b-5b40-4800-ac32-3e6c33e2d588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Display the image\n",
    "# cv2.imshow('image', img)\n",
    "# cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d094c6db-e398-4ebf-8ef5-e8a5c22fbd65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82601746-fd45-4908-ab1d-fde51f809192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import KFold\n",
    "# from keras.callbacks import EarlyStopping\n",
    "\n",
    "# # Define the number of folds for cross-validation\n",
    "# k = 5\n",
    "\n",
    "# # Define the input shape and number of classes for the model\n",
    "# input_shape = (img_height, img_width, channel_dimension)\n",
    "# num_classes = 3\n",
    "\n",
    "# # Define the early stopping callback\n",
    "# early_stop = EarlyStopping(monitor='val_loss', patience=3, verbose=1)\n",
    "\n",
    "# # Create the KFold object\n",
    "# kf = KFold(n_splits=k)\n",
    "\n",
    "\n",
    "# # Loop over the folds\n",
    "# for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
    "\n",
    "#     # Define the model\n",
    "#     model = construct_model(input_shape, num_classes, name='model_fold{}'.format(fold+1))\n",
    "    \n",
    "#     # Compile the model\n",
    "#     model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "#     # Define the training and validation data for this fold\n",
    "#     X_train, y_train = X[train_idx], y[train_idx]\n",
    "#     X_val, y_val = X[val_idx], y[val_idx]\n",
    "    \n",
    "#     # Train the model on this fold\n",
    "#     model.fit(X_train, y_train, validation_data=(X_val, y_val), batch_size=32, epochs=10, callbacks=[early_stop])\n",
    "    \n",
    "#     # Evaluate the model on the validation set for this fold\n",
    "#     score = model.evaluate(X_val, y_val, verbose=0)\n",
    "#     print('Fold {} validation accuracy: {:.4f}'.format(fold+1, score[1]))\n",
    "    \n",
    "#     # Save the model for this fold\n",
    "#     model.save('model_fold{}.h5'.format(fold+1))\n",
    "    \n",
    "# # Compute the average validation accuracy over all folds\n",
    "# avg_val_acc = np.mean([score[1] for score in scores])\n",
    "# print('Average validation accuracy: {:.4f}'.format(avg_val_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6ede52-3664-4916-bcdd-c5b22ee21502",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698bc900-f9d3-464b-96ff-6c99a365e6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Perform data augmentation\n",
    "# # Define the augmentation pipeline\n",
    "# augmentation_pipeline = iaa.Sequential([\n",
    "#     iaa.Fliplr(0.5), # flip horizontally with a probability of 0.5\n",
    "#     iaa.Crop(percent=(0, 0.1)), # crop by up to 10% of the image width/height\n",
    "#     iaa.Sometimes(0.5, iaa.GaussianBlur(sigma=(0, 0.5))), # apply Gaussian blur with a probability of 0.5\n",
    "#     # iaa.Affine(rotate=(-10, 10)) # rotate by up to 10 degrees\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b6319a-356a-4e8f-8269-978eac0b0b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Apply the augmentation pipeline to the training set\n",
    "# augmented_train_images = []\n",
    "# for image in train_images:\n",
    "#     # Apply the same augmentation operation to both the image and its corresponding annotation\n",
    "#     augmented_image = augmentation_pipeline(image=image)\n",
    "#     augmented_train_images.append(augmented_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bffc8df-2ee6-4849-a02d-2f485f0150cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert the augmented training set back to numpy arrays\n",
    "# train_images = np.array(augmented_train_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f20527b-6f40-48c7-8e62-ba4026435725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_random_frames(train_images, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ef17f8-dc33-4276-8a8b-54251098954b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b742b05-ff61-43b7-ae37-6b41e8f71cbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344b1b7e-c6e9-4eaa-b848-4132d926f7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model by calling the function\n",
    "model = construct_model(input_shape, num_classes, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1e6242-13d6-425a-9622-52b07971c65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot model architecture\n",
    "# plot_model(model, to_file='model_plot.png', show_shapes=False, show_layer_names=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f4243f-94af-4c75-a566-1f988e0dd98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import visualkeras\n",
    "# from PIL import ImageFont\n",
    "# visualkeras.layered_view(model, legend=True)\n",
    "# # Define the directory name and the plot name\n",
    "# dir_name = \"/home/dmc/Desktop/kostas/direct-Behavior-prediction-from-miniscope-calcium-imaging-using-convolutional-neural-networks/src/V3/output/architecture\"\n",
    "# plot_name = f\"{model_version}_architecture\"\n",
    "# ann_viz(model, view=True, filename=plot_name, title=\"CNN — \"+str(name)+\" — Simple Architecture\")\n",
    "# plot_path = os.path.join(dir_name, f\"{plot_name}.png\")\n",
    "# plot = plt.gcf()\n",
    "# plot.savefig(plot_path, dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c74194a-fd68-401f-8f4c-ed15bc1f0f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ann_visualizer.visualize import ann_viz\n",
    "# ann_viz(model, view=True, filename=\"cconstruct_model\", title=\"CNN — Model 1 — Simple Architecture\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349d4ac6-31ab-4c24-bf2d-df06d2c898b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.utils.vis_utils import plot_model\n",
    "# plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True, rankdir='LR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfb4ef1-c333-4486-a73d-2fce0344bbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# changes grayscale to rgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b318f97-56ad-481f-9119-2519d60c7985",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data=(val_images, val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1564a72-c699-4078-ae5f-14611ee53b1b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34067e09-e257-4c2b-960e-aa6f89a0f35b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### K-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea9e0d9-ef17-451e-b4cc-ced220d83bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1eae61-58b9-47c2-990a-696a8be0949f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f45731-5470-4104-b301-0aa3beb324f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1eaade6-3f2d-4027-a84a-fc4a2baae028",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fe6f12d4-99d1-4d15-b87a-ea878ccc710e",
   "metadata": {},
   "source": [
    "We have 24186 images of dimensions 349x374 and the number 1 demonstrates that images are grayscale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696bdf1b-dbc9-457d-a04e-38e047838812",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'model': model,\n",
    "    'tf': tf,\n",
    "    'train_images': train_images,\n",
    "    'train_labels': train_labels,\n",
    "    'epochs': epochs,\n",
    "    'batch_size': batch_size,\n",
    "    'validation_data': validation_data,\n",
    "    'val_images': validation_data[0],\n",
    "    'val_labels': validation_data[1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfd3f9b-9f48-4607-a5f2-362d44d9a305",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"/home/dmc/Desktop/kostas/direct-Behavior-prediction-from-miniscope-calcium-imaging-using-convolutional-neural-networks/src/V3/output/pickles\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e726aa57-83bd-4e37-8b6f-95c0939e02d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "history = model_execution(params, save_dir, model_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e26df65-7fa4-4a45-81a8-1cfc1fd7d70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Call the send_email function after your model has finished training\n",
    "# sender_email = 'guskikala@gmail.com'\n",
    "# recipient_email = 'guskikala@gmail.com'\n",
    "# subject = 'CNN Model Training Completed'\n",
    "# message = 'Your CNN model training is complete!'\n",
    "\n",
    "# send_email(sender_email, recipient_email, subject, message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f195ed0-ea69-4425-9a91-d731367d0825",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52ebf8a-fd97-468e-b88b-5b7d78273303",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efefee39-6ca2-4e16-8498-7e8e3f4b4fd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e702dbdd-02df-44b8-beed-a25139174526",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store model\n",
    "%store history\n",
    "%store name\n",
    "%store comment\n",
    "%store experiment_ID\n",
    "%store save_dir\n",
    "%store model_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb53f97-02c8-459a-b1ec-1041f808db98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_training_info(model, history, video_name, comment, experiment_ID, save_dir, f1_score=f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e43045-42d0-4f41-8add-ac3e5b08dd82",
   "metadata": {},
   "source": [
    "High bias: If the training accuracy is low, it suggests that the model is underfitting the training data, i.e., it is not complex enough to capture the patterns in the data. In this case, you may need to increase the model's complexity by adding more layers or neurons, or by using a more complex architecture.\n",
    "\n",
    "High variance: If the training accuracy is high but the validation accuracy is low, it suggests that the model is overfitting the training data, i.e., it is memorizing the training data instead of generalizing to new data. In this case, you may need to use regularization techniques like dropout or L2 regularization, or use early stopping to prevent the model from overfitting.\n",
    "\n",
    "Good fit: If the training accuracy and validation accuracy are both high and close to each other, it suggests that the model is neither underfitting nor overfitting the data, i.e., it is generalizing well to new data.\n",
    "\n",
    "Plateauing: If the validation accuracy is no longer increasing as the training set size or epochs increase, it suggests that the model has reached its capacity and adding more data or epochs is unlikely to improve its performance.\n",
    "\n",
    "In general, a model accuracy curve can help you diagnose issues with your model and guide you in selecting appropriate strategies to improve its performance. It can also give you an idea of how much training data or how many epochs you need to achieve good performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7517b857-b550-4fe3-8fa9-086ca8a4e8c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c658eb9-2008-4398-82f9-e34a230b7cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_confusion_matrix(model, images, labels, classes, title):\n",
    "#     # Predict the class labels using the model\n",
    "#     predicted_labels = np.argmax(model.predict(images), axis=1)\n",
    "\n",
    "#     # Compute the confusion matrix using the predicted class labels and the true class labels\n",
    "#     confusion = confusion_matrix(labels, predicted_labels)\n",
    "\n",
    "#     # Plot the confusion matrix\n",
    "#     fig, ax = plt.subplots(figsize=(10,10))\n",
    "#     ax.imshow(confusion)\n",
    "#     ax.set_xticks(np.arange(len(classes)))\n",
    "#     ax.set_yticks(np.arange(len(classes)))\n",
    "#     ax.set_xticklabels(classes)\n",
    "#     ax.set_yticklabels(classes)\n",
    "#     ax.set_xlabel('Predicted')\n",
    "#     ax.set_ylabel('True')\n",
    "#     ax.set_title(title)\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1778f6fe-63fa-443f-922c-6d5399d3a000",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Plot the confusion matrix for the training set\n",
    "# plot_confusion_matrix(model, train_images, train_labels, num_classes, 'Confusion Matrix for Training Set')\n",
    "\n",
    "# # Plot the confusion matrix for the validation set\n",
    "# plot_confusion_matrix(model, val_images, val_labels, num_classes, 'Confusion Matrix for Validation Set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c6d846-2b61-4f68-a2ae-279a57b48e7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312f730b-f468-4a63-b03a-b3a523cb3dd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431ae521-a723-4ec5-8971-719c5445f462",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d33d2a18-cee9-4f83-8e80-e89e1ef699c8",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Reflect on the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c7f0d1-8e62-4c31-8d65-d7e144301753",
   "metadata": {},
   "source": [
    "1. Insufficient data? One calcium video of 24186 frames and with 349x374 dimensions.\n",
    "2. Model architecture not appropriate. Try increasing the number of layers or filters, or adding more complex layers like BatchNormalization, Dropout, or Conv2DTranspose.\n",
    "3. Incorrect data preprocessing\n",
    "4. Incorrect hyperparameters\n",
    "5. Class Imbalance (do oversampling, or undersampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ae0ef6-1f9c-4afe-8c56-dd0563e9f96e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### [Ignore for now]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9706562-e303-41cc-9e85-ec00240b827f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.layers import BatchNormalization\n",
    "# from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# model = Sequential()\n",
    "\n",
    "# model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', input_shape=(img_height, img_width, channel_dimension)))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# model.add(Conv2D(256, kernel_size=(3, 3), activation='relu'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# model.add(Flatten())\n",
    "\n",
    "# model.add(Dense(256, activation='relu'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Dropout(0.5))\n",
    "\n",
    "# model.add(Dense(128, activation='relu'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Dropout(0.5))\n",
    "\n",
    "# model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# # reshape train_images to have 4 dimensions\n",
    "# train_images = np.expand_dims(train_images, axis=-1)\n",
    "\n",
    "# # Reshape train_images to have 4 dimensions\n",
    "# #train_images = np.squeeze(train_images)\n",
    "# # train_images = np.squeeze(train_images, axis=-1)\n",
    "# # train_images = np.squeeze(train_images, axis=-1)\n",
    "# # train_images = np.squeeze(train_images, axis=-1)\n",
    "# # train_images = np.expand_dims(train_images, axis=-1)\n",
    "\n",
    "\n",
    "# # Data augmentation\n",
    "# train_datagen = ImageDataGenerator(rotation_range=10, width_shift_range=0.1, height_shift_range=0.1, \n",
    "#                                    shear_range=0.1, zoom_range=0.1, horizontal_flip=True, fill_mode='nearest')\n",
    "\n",
    "# history = model.fit(train_datagen.flow(train_images, train_labels, batch_size=batch_size),\n",
    "#                     epochs=epochs,\n",
    "#                     steps_per_epoch=len(train_images) // batch_size,\n",
    "#                     shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b192bf-a682-48c0-bcf4-f30847b0da8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ae0843-9623-4e7f-8e88-a647d9406d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reusable snippets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b0684b-764a-418c-a033-ff5fe9ce130b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load calcium video from local environment\n",
    "# with h5py.File('path', 'r') as f:\n",
    "#     video_data = np.array(f['analysis/recording_20211016_163921-PP-BP-MC/data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b34b51-5633-42e3-8399-6842489b6bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading locally\n",
    "# with h5py.File('/Users/konstantinoskalaitzidis/Developer/dmc/thesis_data/20211016_163921_animal1learnday1.h5', 'r') as f:\n",
    "#     print(list(f.keys()))\n",
    "#     behavior_data = np.array(f['per_frame'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22276208-f41e-4589-9672-50de6e38d4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save the model architecture to a JSON file\n",
    "# with open('model_architecture.json', 'w') as f:\n",
    "#     f.write(model.to_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05dc765c-32f7-4a47-bc62-518a371b26b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load the model architecture from the JSON file\n",
    "# with open('model_architecture.json', 'r') as f:\n",
    "#     json_string = f.read()\n",
    "\n",
    "# model_json = model_from_json(json_string)\n",
    "\n",
    "# # print the loaded model summary\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5af676-d833-4e4a-a4c3-3055adb4ce90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mySession = readSessionServer.SessionIterator()\n",
    "# sess = mySession.findSession()\n",
    "# # for sess in mySession.findSessions():\n",
    "# #     print(sess)\n",
    "# if sess.hasBehavior() and sess.hasCalcium():\n",
    "#     behavior = sess.getBehaviorSegmentation(align_with_calcium=True).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3d1a2e-7307-44d7-9863-dd06baee6227",
   "metadata": {
    "tags": []
   },
   "source": [
    "## [Ignore for now] Define hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719e2a1c-7040-4335-8231-bb45e2893d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMG_SIZE = 224\n",
    "# BATCH_SIZE = 64\n",
    "# EPOCHS = 10\n",
    "\n",
    "# MAX_SEQ_LENGTH = 20\n",
    "# NUM_FEATURES = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd45347-075a-4435-bddb-288d8bc3e506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.path.append(\"/Users/konstantinoskalaitzidis/Developer/dmc\")\n",
    "# from readSessionsServer import SessionIterator\n",
    "\n",
    "#TODO: Script to retrieve videos from a list of calcium videos (of the same animal) from the db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941e710a-0385-4e70-b22f-2fc22953ff07",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Dataset preparation and label annotation (feature engineering)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b3d25e-b6b2-423b-9660-33db62493079",
   "metadata": {
    "tags": []
   },
   "source": [
    "### [Ignore for now] Data availale for processing - overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51d84e0-6996-46bb-8d8f-e3e30d7b0250",
   "metadata": {},
   "source": [
    "The following is not going to be used for now but will allow us to have an overview of all the videos I have available to train my CNN model. I expect to have all recordings sessions for each animal as input for the CNN which is going to be trained only based on recordings from the corresponding animal. The data will be split to train/test at some point..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c8f7e6-4142-4d8c-abde-d6aeedf14113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = pd.read_csv(\"train.csv\")\n",
    "# test_df = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# print(f\"Total videos for training: {len(train_df)}\")\n",
    "# print(f\"Total videos for testing: {len(test_df)}\")\n",
    "\n",
    "# train_df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1cc62e-96d3-4a65-aaff-a4f869cff76b",
   "metadata": {},
   "source": [
    "Extract frames from the calcium imaging video and save to directory. Each frame contains spatial information, and the sequence of those frames contains temporal information (the latter is not exploited for now). Maybe also ask for path input from the user to make it reproducible for others.\n",
    "\n",
    "Helpful source: https://keras.io/examples/vision/video_classification/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f96f31b-c607-4a50-bb5f-9ef8fecdd3e3",
   "metadata": {},
   "source": [
    "The number of frames may differ from video to video.\n",
    "The frame rate may also differ from video to video but it should be 20fps for all. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7dcdb7b-7022-485c-9c96-591b3711feab",
   "metadata": {},
   "source": [
    "The duration of each frame depends on the frame rate of the video. If a video has a frame rate of 25 fps, then each frame will have a duration of 1/25th of a second, or approximately 0.04 seconds. The calcium videos use 20fps, while the behavioral recordings are at 60fps. Alignment of these videos will follow shortly. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a3ae92-9853-43c3-bbef-ede04ab1aec4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### [Ignore for now] Fetch all calcium videos from the dmc database and align calcium videos with behavior annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a136ef5-f526-417f-bc03-855d4e07fb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mySession = readSessionServer.SessionIterator()\n",
    "# for sess in mySession.findSessions():\n",
    "#     print(sess)\n",
    "    # if sess.hasBehavior() and sess.hasCalcium():\n",
    "        # behavior = sess.getBehaviorSegmentation(align_with_calcium=True).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5de1e4-345a-4132-95cc-948a88666259",
   "metadata": {
    "tags": []
   },
   "source": [
    "### [Ignore for now] Open calcium video locally, create dir for saving frames and count number of frames with OpenCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c7bceb-62a8-4589-874f-bd05dd118bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the HDF5 file\n",
    "# with h5py.File('/Users/konstantinoskalaitzidis/Developer/dmc/thesis_data/20211016_163921_animal1learnday1.nwb', 'r') as f:\n",
    "#     # Print the keys of the file\n",
    "#     print(list(f.keys()))\n",
    "#     # dataset = f['identifier'][()]\n",
    "#     # print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e9d178-da84-4268-a626-e8ae346556ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where frames from video will be stored after extraction\n",
    "# frames_dir = \"path\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbee6769-dd9e-4d13-bcb3-7b51b960dce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the video using OpenCV and count the number of frames\n",
    "# cap = cv2.VideoCapture(raw_calcium_video_path)\n",
    "# frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "# cap.release()\n",
    "\n",
    "# print(f\"Number of frames in the video: {frame_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bce9c58-3dd8-4fd6-afb3-9dfb9473b873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# video = 'path'\n",
    "\n",
    "# cap = cv2.VideoCapture(video)\n",
    "# frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "# cap.release()\n",
    "\n",
    "# print(f\"Number of frames in the video: {frame_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fed508-e062-4c0e-8c82-5c49e8c5314d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the video file\n",
    "# cap = cv2.VideoCapture(video)\n",
    "\n",
    "# # Get the frame rate of the video\n",
    "# frame_rate = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "# # Release the video capture object\n",
    "# cap.release()\n",
    "\n",
    "# print(f\"Frame rate of the video: {frame_rate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2146d11a-949c-4877-a84c-5d2e762967d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save each frame as one image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787686cd-d9a4-4939-8ab2-7b7b3f8fbffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cap = cv2.VideoCapture(video)\n",
    "\n",
    "# # Loop through the video frames and save each one as an image file\n",
    "# frame_count = 0\n",
    "# while(cap.isOpened()):\n",
    "#     ret, frame = cap.read()\n",
    "#     if ret == False:\n",
    "#         break\n",
    "#     # Save the frame as an image file\n",
    "#     frame_file = os.path.join(frames_dir, \"frame_\" + str(frame_count) + \".jpg\")\n",
    "#     cv2.imwrite(frame_file, frame)\n",
    "#     frame_count += 1\n",
    "\n",
    "# # Close the video file\n",
    "# cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fd9685-0c81-4a69-b3ee-ed6c91b9f7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # define paths\n",
    "# video_path = '/Users/konstantinoskalaitzidis/Developer/dmc/thesis_data/20211016_163921_animal1learnday1.nwb'\n",
    "# train_dir = '/Users/konstantinoskalaitzidis/Developer/dmc/thesis_data/train'\n",
    "# test_dir = '/Users/konstantinoskalaitzidis/Developer/dmc/thesis_data/test'\n",
    "\n",
    "# # define train-test split ratio\n",
    "# train_test_ratio = 0.8\n",
    "\n",
    "# # open video file\n",
    "# cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# # get video frame count\n",
    "# frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "# # create list of frame indices\n",
    "# frame_indices = list(range(frame_count))\n",
    "\n",
    "# # shuffle frame indices\n",
    "# random.shuffle(frame_indices)\n",
    "\n",
    "# # split frame indices into train and test sets\n",
    "# train_frame_indices = frame_indices[:int(frame_count * train_test_ratio)]\n",
    "# test_frame_indices = frame_indices[int(frame_count * train_test_ratio):]\n",
    "\n",
    "# # iterate over frames and save to train or test directory\n",
    "# for i in range(frame_count):\n",
    "#     # read frame\n",
    "#     ret, frame = cap.read()\n",
    "#     if not ret:\n",
    "#         break\n",
    "    \n",
    "#     # save frame to train or test directory\n",
    "#     if i in train_frame_indices:\n",
    "#         cv2.imwrite(os.path.join(train_dir, f'{i}.jpg'), frame)\n",
    "#     else:\n",
    "#         cv2.imwrite(os.path.join(test_dir, f'{i}.jpg'), frame)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_kostas_env",
   "language": "python",
   "name": "new_kostas_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
